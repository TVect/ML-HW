{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml2021spring-hw1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4OdHJRPwmnWx",
        "hJa7fz-2my6G",
        "410gsmOmso9h"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMODmkdaMlypYsNfSpqVMda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TVect/MLTechHW/blob/master/ntu-ml2021spring/hw01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6XynzRjmMjt"
      },
      "source": [
        "# 数据准备"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRUo4A8pgt-J",
        "outputId": "278230ac-f340-409b-f9af-f47074e55159"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "r-sy1_0yj7RN",
        "outputId": "9352314f-95bd-41ba-e914-0d83473d1266"
      },
      "source": [
        "tr_path = 'covid.train.csv'  # path to training data\n",
        "tt_path = 'covid.test.csv'   # path to testing data\n",
        "\n",
        "%cd /content/drive/MyDrive/ml2021spring-hw1/\n",
        "\n",
        "'''\n",
        "!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n",
        "!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ml2021spring-hw1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\\n!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I-iJHFps3D-"
      },
      "source": [
        "# 数据预处理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OdHJRPwmnWx"
      },
      "source": [
        "## Import Some Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb0Y_A7dmXVS"
      },
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "myseed = 42069  # set a random seed for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJa7fz-2my6G"
      },
      "source": [
        "## Some Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLT-8vcYm6zf"
      },
      "source": [
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def plot_learning_curve(loss_record, title=''):\n",
        "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "    total_steps = len(loss_record['train'])\n",
        "    x_1 = range(total_steps)\n",
        "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
        "    figure(figsize=(6, 4))\n",
        "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
        "    plt.ylim(0.0, 5.)\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('MSE loss')\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n",
        "    ''' Plot prediction of your DNN '''\n",
        "    if preds is None or targets is None:\n",
        "        model.eval()\n",
        "        preds, targets = [], []\n",
        "        for x, y in dv_set:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                pred = model(x)\n",
        "                preds.append(pred.detach().cpu())\n",
        "                targets.append(y.detach().cpu())\n",
        "        preds = torch.cat(preds, dim=0).numpy()\n",
        "        targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "    figure(figsize=(5, 5))\n",
        "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
        "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
        "    plt.xlim(-0.2, lim)\n",
        "    plt.ylim(-0.2, lim)\n",
        "    plt.xlabel('ground truth value')\n",
        "    plt.ylabel('predicted value')\n",
        "    plt.title('Ground Truth v.s. Prediction')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sun8aP_nKX2"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVo3ry2ZnM0X"
      },
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
        "    def __init__(self,\n",
        "                 path,\n",
        "                 mode='train',\n",
        "                 target_only=False):\n",
        "        self.mode = mode\n",
        "\n",
        "        # Read data into numpy arrays\n",
        "        with open(path, 'r') as fp:\n",
        "            data = list(csv.reader(fp))\n",
        "            data = np.array(data[1:])[:, 1:].astype(float)\n",
        "        \n",
        "        if not target_only:\n",
        "            feats = list(range(93))\n",
        "        else:\n",
        "            # TODO: Using 40 states & 2 tested_positive features (indices = 57 & 75)\n",
        "            # feats = list(range(40)) + [57, 75]\n",
        "            feats = list(range(40)) + [41, 42, 43, 55, 57, 58, 59, 60, 61, 73, 75, 76, 77, 78, 79, 91]\n",
        "            pass\n",
        "\n",
        "        if mode == 'test':\n",
        "            # Testing data\n",
        "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
        "            data = data[:, feats]\n",
        "            self.data = torch.FloatTensor(data)\n",
        "        else:\n",
        "            # Training data (train/dev sets)\n",
        "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
        "            target = data[:, -1]\n",
        "            data = data[:, feats]\n",
        "            \n",
        "            # Splitting training data into train & dev sets\n",
        "            # if mode == 'train':\n",
        "            #     indices = [i for i in range(len(data)) if i % 10 != 0]\n",
        "            # elif mode == 'dev':\n",
        "            #     indices = [i for i in range(len(data)) if i % 10 == 0]\n",
        "            np.random.seed(10)\n",
        "            random_nums = np.random.rand(len(data))\n",
        "            if mode == 'train':\n",
        "              indices = np.where(random_nums > 0.1)\n",
        "            elif mode == \"dev\":\n",
        "              indices = np.where(random_nums <= 0.1)\n",
        "    \n",
        "            # Convert data into PyTorch tensors\n",
        "            self.data = torch.FloatTensor(data[indices])\n",
        "            self.target = torch.FloatTensor(target[indices])\n",
        "\n",
        "        # Normalize features (you may remove this part to see what will happen)\n",
        "        # self.data[:, 40:] = \\\n",
        "        #     (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) \\\n",
        "        #     / self.data[:, 40:].std(dim=0, keepdim=True)\n",
        "\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\n",
        "              .format(mode, len(self.data), self.dim))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9YmcYptn97v"
      },
      "source": [
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size,\n",
        "        shuffle=(mode == 'train'), drop_last=False,\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
        "    return dataloader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN4jiaPCs8Da"
      },
      "source": [
        "# 模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbxs4aVvoKuV"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL5JIMuboMpg"
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        # Define your neural network here\n",
        "        # TODO: How to modify this model to achieve better performance?\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        # TODO: you may implement L2 regularization here\n",
        "        return self.criterion(pred, target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltQMdIMvoUdo"
      },
      "source": [
        "## Train & Dev & Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkYwHdN3ovMR"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzjNkkjDoYc4"
      },
      "source": [
        "def train(tr_set, dv_set, model, config, device):\n",
        "    ''' DNN training '''\n",
        "\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "    while epoch < n_epochs:\n",
        "        tmp_train_loss = []\n",
        "        model.train()                           # set model to training mode\n",
        "        for x, y in tr_set:                     # iterate through the dataloader\n",
        "            optimizer.zero_grad()               # set gradient to zero\n",
        "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
        "            optimizer.step()                    # update model with optimizer\n",
        "            tmp_train_loss.append(mse_loss.detach().cpu().item())\n",
        "            # loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "\n",
        "        loss_record['train'].extend(tmp_train_loss)\n",
        "        # After each epoch, test your model on the validation (development) set.\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        if dev_mse < min_mse:\n",
        "            # Save model if your model improved\n",
        "            min_mse = dev_mse\n",
        "            # print('...... Saving model (epoch = {:4d}, loss = {:.4f})'\n",
        "            #     .format(epoch + 1, min_mse))\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        epoch += 1\n",
        "        loss_record['dev'].append(dev_mse)\n",
        "        if epoch % 10 == 0:\n",
        "          print(\"epoch = {:4d}, train_loss = {:.4f}, dev_mse = {:.4f}\"\n",
        "                .format(epoch, np.mean(tmp_train_loss), dev_mse))\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
        "            break\n",
        "\n",
        "    print('Finished training after {} epochs. min_mse: {:.4f}'.format(epoch, min_mse))\n",
        "    return min_mse, loss_record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97GNAjbto14j"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpeUpXSnohy6"
      },
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\n",
        "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXeS41WjpJx_"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef7aUap2ozsn"
      },
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfG4ra6dpY4a"
      },
      "source": [
        "### Setup Hyper-parameters¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBtH5VAkpMDZ"
      },
      "source": [
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "# target_only = False                   # TODO: Using 40 states & 2 tested_positive features\n",
        "target_only = True\n",
        "\n",
        "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
        "config = {\n",
        "    'n_epochs': 8000,                # maximum number of epochs\n",
        "    # 'batch_size': 270,               # mini-batch size for dataloader\n",
        "    'batch_size': 2430,\n",
        "    # 'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optimizer': 'Adam',\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "    #     'weight_decay': 0.05,\n",
        "        # 'lr': 0.001,                 # learning rate of SGD\n",
        "        'lr': 0.0005,\n",
        "        # 'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 5000,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBi6HZxVph80"
      },
      "source": [
        "### Load data and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HGUflkDpdfU",
        "outputId": "ff43b8f2-c480-455e-bac6-54523996c968"
      },
      "source": [
        "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
        "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
        "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2409 samples found, each dim = 56)\n",
            "Finished reading the dev set of COVID19 Dataset (291 samples found, each dim = 56)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdmiNyBfrkMY"
      },
      "source": [
        "### Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6-uBtEwqte7",
        "outputId": "e8bc2687-176c-4b57-cb6e-b82db060924a"
      },
      "source": [
        "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device\n",
        "\n",
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch =   10, train_loss = 283.5011, dev_mse = 250.9758\n",
            "epoch =   20, train_loss = 164.9359, dev_mse = 143.8042\n",
            "epoch =   30, train_loss = 92.0698, dev_mse = 79.2503\n",
            "epoch =   40, train_loss = 52.7948, dev_mse = 45.5537\n",
            "epoch =   50, train_loss = 33.6973, dev_mse = 29.8757\n",
            "epoch =   60, train_loss = 26.2414, dev_mse = 24.3667\n",
            "epoch =   70, train_loss = 23.9955, dev_mse = 23.0382\n",
            "epoch =   80, train_loss = 23.2378, dev_mse = 22.6230\n",
            "epoch =   90, train_loss = 22.5069, dev_mse = 21.9718\n",
            "epoch =  100, train_loss = 21.6073, dev_mse = 21.0633\n",
            "epoch =  110, train_loss = 20.6600, dev_mse = 20.1419\n",
            "epoch =  120, train_loss = 19.7734, dev_mse = 19.3106\n",
            "epoch =  130, train_loss = 18.8003, dev_mse = 18.3200\n",
            "epoch =  140, train_loss = 17.7444, dev_mse = 17.3680\n",
            "epoch =  150, train_loss = 16.5943, dev_mse = 16.3742\n",
            "epoch =  160, train_loss = 15.4556, dev_mse = 15.3329\n",
            "epoch =  170, train_loss = 14.3543, dev_mse = 14.3256\n",
            "epoch =  180, train_loss = 13.3054, dev_mse = 13.3682\n",
            "epoch =  190, train_loss = 12.3298, dev_mse = 12.4718\n",
            "epoch =  200, train_loss = 11.4416, dev_mse = 11.6547\n",
            "epoch =  210, train_loss = 10.6519, dev_mse = 10.9334\n",
            "epoch =  220, train_loss = 9.9616, dev_mse = 10.3070\n",
            "epoch =  230, train_loss = 9.3672, dev_mse = 9.7687\n",
            "epoch =  240, train_loss = 8.8612, dev_mse = 9.3092\n",
            "epoch =  250, train_loss = 8.4330, dev_mse = 8.9162\n",
            "epoch =  260, train_loss = 8.0693, dev_mse = 8.5786\n",
            "epoch =  270, train_loss = 7.7570, dev_mse = 8.2848\n",
            "epoch =  280, train_loss = 7.4843, dev_mse = 8.0243\n",
            "epoch =  290, train_loss = 7.2408, dev_mse = 7.7884\n",
            "epoch =  300, train_loss = 7.0174, dev_mse = 7.5677\n",
            "epoch =  310, train_loss = 6.8080, dev_mse = 7.3567\n",
            "epoch =  320, train_loss = 6.6084, dev_mse = 7.1530\n",
            "epoch =  330, train_loss = 6.4145, dev_mse = 6.9526\n",
            "epoch =  340, train_loss = 6.2250, dev_mse = 6.7557\n",
            "epoch =  350, train_loss = 6.0383, dev_mse = 6.5632\n",
            "epoch =  360, train_loss = 5.8507, dev_mse = 6.3662\n",
            "epoch =  370, train_loss = 5.6654, dev_mse = 6.1651\n",
            "epoch =  380, train_loss = 5.4861, dev_mse = 5.9659\n",
            "epoch =  390, train_loss = 5.3124, dev_mse = 5.7708\n",
            "epoch =  400, train_loss = 5.1439, dev_mse = 5.5821\n",
            "epoch =  410, train_loss = 4.9787, dev_mse = 5.3970\n",
            "epoch =  420, train_loss = 4.8167, dev_mse = 5.2161\n",
            "epoch =  430, train_loss = 4.6571, dev_mse = 5.0381\n",
            "epoch =  440, train_loss = 4.4997, dev_mse = 4.8631\n",
            "epoch =  450, train_loss = 4.3446, dev_mse = 4.6910\n",
            "epoch =  460, train_loss = 4.1917, dev_mse = 4.5202\n",
            "epoch =  470, train_loss = 4.0413, dev_mse = 4.3523\n",
            "epoch =  480, train_loss = 3.8933, dev_mse = 4.1868\n",
            "epoch =  490, train_loss = 3.7482, dev_mse = 4.0248\n",
            "epoch =  500, train_loss = 3.6060, dev_mse = 3.8655\n",
            "epoch =  510, train_loss = 3.4670, dev_mse = 3.7103\n",
            "epoch =  520, train_loss = 3.3314, dev_mse = 3.5592\n",
            "epoch =  530, train_loss = 3.1995, dev_mse = 3.4122\n",
            "epoch =  540, train_loss = 3.0715, dev_mse = 3.2695\n",
            "epoch =  550, train_loss = 2.9477, dev_mse = 3.1315\n",
            "epoch =  560, train_loss = 2.8282, dev_mse = 2.9983\n",
            "epoch =  570, train_loss = 2.7133, dev_mse = 2.8701\n",
            "epoch =  580, train_loss = 2.6034, dev_mse = 2.7480\n",
            "epoch =  590, train_loss = 2.4984, dev_mse = 2.6312\n",
            "epoch =  600, train_loss = 2.3985, dev_mse = 2.5200\n",
            "epoch =  610, train_loss = 2.3037, dev_mse = 2.4143\n",
            "epoch =  620, train_loss = 2.2141, dev_mse = 2.3143\n",
            "epoch =  630, train_loss = 2.1294, dev_mse = 2.2197\n",
            "epoch =  640, train_loss = 2.0498, dev_mse = 2.1308\n",
            "epoch =  650, train_loss = 1.9751, dev_mse = 2.0475\n",
            "epoch =  660, train_loss = 1.9055, dev_mse = 1.9694\n",
            "epoch =  670, train_loss = 1.8405, dev_mse = 1.8966\n",
            "epoch =  680, train_loss = 1.7801, dev_mse = 1.8290\n",
            "epoch =  690, train_loss = 1.7242, dev_mse = 1.7666\n",
            "epoch =  700, train_loss = 1.6727, dev_mse = 1.7095\n",
            "epoch =  710, train_loss = 1.6252, dev_mse = 1.6570\n",
            "epoch =  720, train_loss = 1.5817, dev_mse = 1.6090\n",
            "epoch =  730, train_loss = 1.5422, dev_mse = 1.5651\n",
            "epoch =  740, train_loss = 1.5062, dev_mse = 1.5248\n",
            "epoch =  750, train_loss = 1.4736, dev_mse = 1.4878\n",
            "epoch =  760, train_loss = 1.4442, dev_mse = 1.4540\n",
            "epoch =  770, train_loss = 1.4177, dev_mse = 1.4233\n",
            "epoch =  780, train_loss = 1.3938, dev_mse = 1.3955\n",
            "epoch =  790, train_loss = 1.3722, dev_mse = 1.3702\n",
            "epoch =  800, train_loss = 1.3528, dev_mse = 1.3473\n",
            "epoch =  810, train_loss = 1.3352, dev_mse = 1.3270\n",
            "epoch =  820, train_loss = 1.3193, dev_mse = 1.3085\n",
            "epoch =  830, train_loss = 1.3048, dev_mse = 1.2924\n",
            "epoch =  840, train_loss = 1.2920, dev_mse = 1.2778\n",
            "epoch =  850, train_loss = 1.2804, dev_mse = 1.2648\n",
            "epoch =  860, train_loss = 1.2701, dev_mse = 1.2533\n",
            "epoch =  870, train_loss = 1.2610, dev_mse = 1.2433\n",
            "epoch =  880, train_loss = 1.2529, dev_mse = 1.2341\n",
            "epoch =  890, train_loss = 1.2456, dev_mse = 1.2261\n",
            "epoch =  900, train_loss = 1.2392, dev_mse = 1.2190\n",
            "epoch =  910, train_loss = 1.2334, dev_mse = 1.2126\n",
            "epoch =  920, train_loss = 1.2282, dev_mse = 1.2068\n",
            "epoch =  930, train_loss = 1.2234, dev_mse = 1.2016\n",
            "epoch =  940, train_loss = 1.2191, dev_mse = 1.1970\n",
            "epoch =  950, train_loss = 1.2150, dev_mse = 1.1928\n",
            "epoch =  960, train_loss = 1.2111, dev_mse = 1.1889\n",
            "epoch =  970, train_loss = 1.2075, dev_mse = 1.1854\n",
            "epoch =  980, train_loss = 1.2041, dev_mse = 1.1827\n",
            "epoch =  990, train_loss = 1.2009, dev_mse = 1.1801\n",
            "epoch = 1000, train_loss = 1.1980, dev_mse = 1.1777\n",
            "epoch = 1010, train_loss = 1.1953, dev_mse = 1.1756\n",
            "epoch = 1020, train_loss = 1.1928, dev_mse = 1.1737\n",
            "epoch = 1030, train_loss = 1.1903, dev_mse = 1.1719\n",
            "epoch = 1040, train_loss = 1.1879, dev_mse = 1.1701\n",
            "epoch = 1050, train_loss = 1.1857, dev_mse = 1.1686\n",
            "epoch = 1060, train_loss = 1.1833, dev_mse = 1.1674\n",
            "epoch = 1070, train_loss = 1.1808, dev_mse = 1.1664\n",
            "epoch = 1080, train_loss = 1.1783, dev_mse = 1.1653\n",
            "epoch = 1090, train_loss = 1.1757, dev_mse = 1.1641\n",
            "epoch = 1100, train_loss = 1.1730, dev_mse = 1.1631\n",
            "epoch = 1110, train_loss = 1.1701, dev_mse = 1.1621\n",
            "epoch = 1120, train_loss = 1.1673, dev_mse = 1.1610\n",
            "epoch = 1130, train_loss = 1.1651, dev_mse = 1.1598\n",
            "epoch = 1140, train_loss = 1.1630, dev_mse = 1.1585\n",
            "epoch = 1150, train_loss = 1.1608, dev_mse = 1.1566\n",
            "epoch = 1160, train_loss = 1.1587, dev_mse = 1.1548\n",
            "epoch = 1170, train_loss = 1.1568, dev_mse = 1.1530\n",
            "epoch = 1180, train_loss = 1.1550, dev_mse = 1.1511\n",
            "epoch = 1190, train_loss = 1.1533, dev_mse = 1.1492\n",
            "epoch = 1200, train_loss = 1.1517, dev_mse = 1.1473\n",
            "epoch = 1210, train_loss = 1.1502, dev_mse = 1.1455\n",
            "epoch = 1220, train_loss = 1.1487, dev_mse = 1.1437\n",
            "epoch = 1230, train_loss = 1.1473, dev_mse = 1.1419\n",
            "epoch = 1240, train_loss = 1.1459, dev_mse = 1.1402\n",
            "epoch = 1250, train_loss = 1.1445, dev_mse = 1.1384\n",
            "epoch = 1260, train_loss = 1.1432, dev_mse = 1.1367\n",
            "epoch = 1270, train_loss = 1.1419, dev_mse = 1.1351\n",
            "epoch = 1280, train_loss = 1.1407, dev_mse = 1.1335\n",
            "epoch = 1290, train_loss = 1.1394, dev_mse = 1.1320\n",
            "epoch = 1300, train_loss = 1.1381, dev_mse = 1.1302\n",
            "epoch = 1310, train_loss = 1.1369, dev_mse = 1.1288\n",
            "epoch = 1320, train_loss = 1.1357, dev_mse = 1.1272\n",
            "epoch = 1330, train_loss = 1.1344, dev_mse = 1.1257\n",
            "epoch = 1340, train_loss = 1.1332, dev_mse = 1.1243\n",
            "epoch = 1350, train_loss = 1.1321, dev_mse = 1.1228\n",
            "epoch = 1360, train_loss = 1.1309, dev_mse = 1.1214\n",
            "epoch = 1370, train_loss = 1.1297, dev_mse = 1.1200\n",
            "epoch = 1380, train_loss = 1.1286, dev_mse = 1.1186\n",
            "epoch = 1390, train_loss = 1.1274, dev_mse = 1.1172\n",
            "epoch = 1400, train_loss = 1.1263, dev_mse = 1.1159\n",
            "epoch = 1410, train_loss = 1.1252, dev_mse = 1.1145\n",
            "epoch = 1420, train_loss = 1.1240, dev_mse = 1.1131\n",
            "epoch = 1430, train_loss = 1.1229, dev_mse = 1.1118\n",
            "epoch = 1440, train_loss = 1.1218, dev_mse = 1.1104\n",
            "epoch = 1450, train_loss = 1.1207, dev_mse = 1.1091\n",
            "epoch = 1460, train_loss = 1.1196, dev_mse = 1.1077\n",
            "epoch = 1470, train_loss = 1.1185, dev_mse = 1.1064\n",
            "epoch = 1480, train_loss = 1.1174, dev_mse = 1.1051\n",
            "epoch = 1490, train_loss = 1.1163, dev_mse = 1.1038\n",
            "epoch = 1500, train_loss = 1.1152, dev_mse = 1.1025\n",
            "epoch = 1510, train_loss = 1.1142, dev_mse = 1.1012\n",
            "epoch = 1520, train_loss = 1.1131, dev_mse = 1.0999\n",
            "epoch = 1530, train_loss = 1.1120, dev_mse = 1.0986\n",
            "epoch = 1540, train_loss = 1.1110, dev_mse = 1.0973\n",
            "epoch = 1550, train_loss = 1.1099, dev_mse = 1.0961\n",
            "epoch = 1560, train_loss = 1.1089, dev_mse = 1.0947\n",
            "epoch = 1570, train_loss = 1.1078, dev_mse = 1.0935\n",
            "epoch = 1580, train_loss = 1.1068, dev_mse = 1.0922\n",
            "epoch = 1590, train_loss = 1.1057, dev_mse = 1.0910\n",
            "epoch = 1600, train_loss = 1.1047, dev_mse = 1.0897\n",
            "epoch = 1610, train_loss = 1.1036, dev_mse = 1.0885\n",
            "epoch = 1620, train_loss = 1.1026, dev_mse = 1.0873\n",
            "epoch = 1630, train_loss = 1.1016, dev_mse = 1.0860\n",
            "epoch = 1640, train_loss = 1.1006, dev_mse = 1.0848\n",
            "epoch = 1650, train_loss = 1.0995, dev_mse = 1.0836\n",
            "epoch = 1660, train_loss = 1.0985, dev_mse = 1.0823\n",
            "epoch = 1670, train_loss = 1.0975, dev_mse = 1.0812\n",
            "epoch = 1680, train_loss = 1.0965, dev_mse = 1.0800\n",
            "epoch = 1690, train_loss = 1.0955, dev_mse = 1.0788\n",
            "epoch = 1700, train_loss = 1.0944, dev_mse = 1.0776\n",
            "epoch = 1710, train_loss = 1.0934, dev_mse = 1.0763\n",
            "epoch = 1720, train_loss = 1.0924, dev_mse = 1.0751\n",
            "epoch = 1730, train_loss = 1.0914, dev_mse = 1.0738\n",
            "epoch = 1740, train_loss = 1.0904, dev_mse = 1.0726\n",
            "epoch = 1750, train_loss = 1.0894, dev_mse = 1.0714\n",
            "epoch = 1760, train_loss = 1.0884, dev_mse = 1.0701\n",
            "epoch = 1770, train_loss = 1.0874, dev_mse = 1.0689\n",
            "epoch = 1780, train_loss = 1.0864, dev_mse = 1.0677\n",
            "epoch = 1790, train_loss = 1.0854, dev_mse = 1.0665\n",
            "epoch = 1800, train_loss = 1.0844, dev_mse = 1.0654\n",
            "epoch = 1810, train_loss = 1.0835, dev_mse = 1.0643\n",
            "epoch = 1820, train_loss = 1.0825, dev_mse = 1.0631\n",
            "epoch = 1830, train_loss = 1.0815, dev_mse = 1.0621\n",
            "epoch = 1840, train_loss = 1.0805, dev_mse = 1.0608\n",
            "epoch = 1850, train_loss = 1.0795, dev_mse = 1.0596\n",
            "epoch = 1860, train_loss = 1.0785, dev_mse = 1.0585\n",
            "epoch = 1870, train_loss = 1.0775, dev_mse = 1.0574\n",
            "epoch = 1880, train_loss = 1.0765, dev_mse = 1.0558\n",
            "epoch = 1890, train_loss = 1.0754, dev_mse = 1.0550\n",
            "epoch = 1900, train_loss = 1.0740, dev_mse = 1.0544\n",
            "epoch = 1910, train_loss = 1.0729, dev_mse = 1.0540\n",
            "epoch = 1920, train_loss = 1.0719, dev_mse = 1.0533\n",
            "epoch = 1930, train_loss = 1.0709, dev_mse = 1.0521\n",
            "epoch = 1940, train_loss = 1.0699, dev_mse = 1.0505\n",
            "epoch = 1950, train_loss = 1.0689, dev_mse = 1.0489\n",
            "epoch = 1960, train_loss = 1.0679, dev_mse = 1.0482\n",
            "epoch = 1970, train_loss = 1.0670, dev_mse = 1.0471\n",
            "epoch = 1980, train_loss = 1.0660, dev_mse = 1.0460\n",
            "epoch = 1990, train_loss = 1.0650, dev_mse = 1.0448\n",
            "epoch = 2000, train_loss = 1.0641, dev_mse = 1.0438\n",
            "epoch = 2010, train_loss = 1.0631, dev_mse = 1.0425\n",
            "epoch = 2020, train_loss = 1.0621, dev_mse = 1.0413\n",
            "epoch = 2030, train_loss = 1.0612, dev_mse = 1.0403\n",
            "epoch = 2040, train_loss = 1.0602, dev_mse = 1.0392\n",
            "epoch = 2050, train_loss = 1.0592, dev_mse = 1.0381\n",
            "epoch = 2060, train_loss = 1.0583, dev_mse = 1.0369\n",
            "epoch = 2070, train_loss = 1.0573, dev_mse = 1.0358\n",
            "epoch = 2080, train_loss = 1.0563, dev_mse = 1.0346\n",
            "epoch = 2090, train_loss = 1.0554, dev_mse = 1.0337\n",
            "epoch = 2100, train_loss = 1.0544, dev_mse = 1.0325\n",
            "epoch = 2110, train_loss = 1.0535, dev_mse = 1.0315\n",
            "epoch = 2120, train_loss = 1.0525, dev_mse = 1.0302\n",
            "epoch = 2130, train_loss = 1.0515, dev_mse = 1.0292\n",
            "epoch = 2140, train_loss = 1.0506, dev_mse = 1.0282\n",
            "epoch = 2150, train_loss = 1.0496, dev_mse = 1.0269\n",
            "epoch = 2160, train_loss = 1.0487, dev_mse = 1.0260\n",
            "epoch = 2170, train_loss = 1.0477, dev_mse = 1.0249\n",
            "epoch = 2180, train_loss = 1.0468, dev_mse = 1.0238\n",
            "epoch = 2190, train_loss = 1.0458, dev_mse = 1.0227\n",
            "epoch = 2200, train_loss = 1.0449, dev_mse = 1.0216\n",
            "epoch = 2210, train_loss = 1.0439, dev_mse = 1.0205\n",
            "epoch = 2220, train_loss = 1.0430, dev_mse = 1.0194\n",
            "epoch = 2230, train_loss = 1.0420, dev_mse = 1.0183\n",
            "epoch = 2240, train_loss = 1.0411, dev_mse = 1.0172\n",
            "epoch = 2250, train_loss = 1.0402, dev_mse = 1.0161\n",
            "epoch = 2260, train_loss = 1.0392, dev_mse = 1.0151\n",
            "epoch = 2270, train_loss = 1.0383, dev_mse = 1.0141\n",
            "epoch = 2280, train_loss = 1.0373, dev_mse = 1.0132\n",
            "epoch = 2290, train_loss = 1.0364, dev_mse = 1.0117\n",
            "epoch = 2300, train_loss = 1.0355, dev_mse = 1.0107\n",
            "epoch = 2310, train_loss = 1.0345, dev_mse = 1.0098\n",
            "epoch = 2320, train_loss = 1.0336, dev_mse = 1.0089\n",
            "epoch = 2330, train_loss = 1.0327, dev_mse = 1.0076\n",
            "epoch = 2340, train_loss = 1.0317, dev_mse = 1.0066\n",
            "epoch = 2350, train_loss = 1.0308, dev_mse = 1.0056\n",
            "epoch = 2360, train_loss = 1.0299, dev_mse = 1.0045\n",
            "epoch = 2370, train_loss = 1.0289, dev_mse = 1.0034\n",
            "epoch = 2380, train_loss = 1.0280, dev_mse = 1.0024\n",
            "epoch = 2390, train_loss = 1.0271, dev_mse = 1.0013\n",
            "epoch = 2400, train_loss = 1.0262, dev_mse = 1.0003\n",
            "epoch = 2410, train_loss = 1.0252, dev_mse = 0.9992\n",
            "epoch = 2420, train_loss = 1.0243, dev_mse = 0.9981\n",
            "epoch = 2430, train_loss = 1.0234, dev_mse = 0.9971\n",
            "epoch = 2440, train_loss = 1.0225, dev_mse = 0.9960\n",
            "epoch = 2450, train_loss = 1.0215, dev_mse = 0.9950\n",
            "epoch = 2460, train_loss = 1.0206, dev_mse = 0.9939\n",
            "epoch = 2470, train_loss = 1.0197, dev_mse = 0.9928\n",
            "epoch = 2480, train_loss = 1.0187, dev_mse = 0.9921\n",
            "epoch = 2490, train_loss = 1.0178, dev_mse = 0.9907\n",
            "epoch = 2500, train_loss = 1.0168, dev_mse = 0.9899\n",
            "epoch = 2510, train_loss = 1.0159, dev_mse = 0.9885\n",
            "epoch = 2520, train_loss = 1.0150, dev_mse = 0.9878\n",
            "epoch = 2530, train_loss = 1.0141, dev_mse = 0.9866\n",
            "epoch = 2540, train_loss = 1.0132, dev_mse = 0.9857\n",
            "epoch = 2550, train_loss = 1.0122, dev_mse = 0.9846\n",
            "epoch = 2560, train_loss = 1.0113, dev_mse = 0.9837\n",
            "epoch = 2570, train_loss = 1.0104, dev_mse = 0.9825\n",
            "epoch = 2580, train_loss = 1.0095, dev_mse = 0.9816\n",
            "epoch = 2590, train_loss = 1.0086, dev_mse = 0.9803\n",
            "epoch = 2600, train_loss = 1.0077, dev_mse = 0.9793\n",
            "epoch = 2610, train_loss = 1.0068, dev_mse = 0.9784\n",
            "epoch = 2620, train_loss = 1.0059, dev_mse = 0.9771\n",
            "epoch = 2630, train_loss = 1.0050, dev_mse = 0.9762\n",
            "epoch = 2640, train_loss = 1.0041, dev_mse = 0.9751\n",
            "epoch = 2650, train_loss = 1.0032, dev_mse = 0.9743\n",
            "epoch = 2660, train_loss = 1.0023, dev_mse = 0.9732\n",
            "epoch = 2670, train_loss = 1.0014, dev_mse = 0.9720\n",
            "epoch = 2680, train_loss = 1.0005, dev_mse = 0.9710\n",
            "epoch = 2690, train_loss = 0.9996, dev_mse = 0.9701\n",
            "epoch = 2700, train_loss = 0.9987, dev_mse = 0.9691\n",
            "epoch = 2710, train_loss = 0.9978, dev_mse = 0.9680\n",
            "epoch = 2720, train_loss = 0.9969, dev_mse = 0.9669\n",
            "epoch = 2730, train_loss = 0.9961, dev_mse = 0.9660\n",
            "epoch = 2740, train_loss = 0.9952, dev_mse = 0.9649\n",
            "epoch = 2750, train_loss = 0.9943, dev_mse = 0.9639\n",
            "epoch = 2760, train_loss = 0.9934, dev_mse = 0.9628\n",
            "epoch = 2770, train_loss = 0.9926, dev_mse = 0.9618\n",
            "epoch = 2780, train_loss = 0.9917, dev_mse = 0.9608\n",
            "epoch = 2790, train_loss = 0.9908, dev_mse = 0.9597\n",
            "epoch = 2800, train_loss = 0.9899, dev_mse = 0.9587\n",
            "epoch = 2810, train_loss = 0.9891, dev_mse = 0.9578\n",
            "epoch = 2820, train_loss = 0.9882, dev_mse = 0.9567\n",
            "epoch = 2830, train_loss = 0.9874, dev_mse = 0.9556\n",
            "epoch = 2840, train_loss = 0.9865, dev_mse = 0.9548\n",
            "epoch = 2850, train_loss = 0.9856, dev_mse = 0.9538\n",
            "epoch = 2860, train_loss = 0.9848, dev_mse = 0.9525\n",
            "epoch = 2870, train_loss = 0.9839, dev_mse = 0.9517\n",
            "epoch = 2880, train_loss = 0.9831, dev_mse = 0.9508\n",
            "epoch = 2890, train_loss = 0.9822, dev_mse = 0.9497\n",
            "epoch = 2900, train_loss = 0.9813, dev_mse = 0.9488\n",
            "epoch = 2910, train_loss = 0.9805, dev_mse = 0.9479\n",
            "epoch = 2920, train_loss = 0.9796, dev_mse = 0.9469\n",
            "epoch = 2930, train_loss = 0.9788, dev_mse = 0.9461\n",
            "epoch = 2940, train_loss = 0.9780, dev_mse = 0.9450\n",
            "epoch = 2950, train_loss = 0.9771, dev_mse = 0.9441\n",
            "epoch = 2960, train_loss = 0.9763, dev_mse = 0.9432\n",
            "epoch = 2970, train_loss = 0.9755, dev_mse = 0.9423\n",
            "epoch = 2980, train_loss = 0.9746, dev_mse = 0.9414\n",
            "epoch = 2990, train_loss = 0.9738, dev_mse = 0.9404\n",
            "epoch = 3000, train_loss = 0.9730, dev_mse = 0.9394\n",
            "epoch = 3010, train_loss = 0.9722, dev_mse = 0.9385\n",
            "epoch = 3020, train_loss = 0.9714, dev_mse = 0.9375\n",
            "epoch = 3030, train_loss = 0.9705, dev_mse = 0.9367\n",
            "epoch = 3040, train_loss = 0.9697, dev_mse = 0.9357\n",
            "epoch = 3050, train_loss = 0.9689, dev_mse = 0.9349\n",
            "epoch = 3060, train_loss = 0.9681, dev_mse = 0.9339\n",
            "epoch = 3070, train_loss = 0.9673, dev_mse = 0.9329\n",
            "epoch = 3080, train_loss = 0.9665, dev_mse = 0.9320\n",
            "epoch = 3090, train_loss = 0.9657, dev_mse = 0.9310\n",
            "epoch = 3100, train_loss = 0.9649, dev_mse = 0.9301\n",
            "epoch = 3110, train_loss = 0.9641, dev_mse = 0.9291\n",
            "epoch = 3120, train_loss = 0.9633, dev_mse = 0.9280\n",
            "epoch = 3130, train_loss = 0.9625, dev_mse = 0.9270\n",
            "epoch = 3140, train_loss = 0.9617, dev_mse = 0.9262\n",
            "epoch = 3150, train_loss = 0.9609, dev_mse = 0.9255\n",
            "epoch = 3160, train_loss = 0.9602, dev_mse = 0.9243\n",
            "epoch = 3170, train_loss = 0.9594, dev_mse = 0.9235\n",
            "epoch = 3180, train_loss = 0.9586, dev_mse = 0.9227\n",
            "epoch = 3190, train_loss = 0.9579, dev_mse = 0.9218\n",
            "epoch = 3200, train_loss = 0.9571, dev_mse = 0.9207\n",
            "epoch = 3210, train_loss = 0.9563, dev_mse = 0.9198\n",
            "epoch = 3220, train_loss = 0.9556, dev_mse = 0.9189\n",
            "epoch = 3230, train_loss = 0.9548, dev_mse = 0.9183\n",
            "epoch = 3240, train_loss = 0.9541, dev_mse = 0.9172\n",
            "epoch = 3250, train_loss = 0.9534, dev_mse = 0.9163\n",
            "epoch = 3260, train_loss = 0.9526, dev_mse = 0.9155\n",
            "epoch = 3270, train_loss = 0.9519, dev_mse = 0.9146\n",
            "epoch = 3280, train_loss = 0.9512, dev_mse = 0.9136\n",
            "epoch = 3290, train_loss = 0.9504, dev_mse = 0.9129\n",
            "epoch = 3300, train_loss = 0.9497, dev_mse = 0.9122\n",
            "epoch = 3310, train_loss = 0.9490, dev_mse = 0.9112\n",
            "epoch = 3320, train_loss = 0.9483, dev_mse = 0.9104\n",
            "epoch = 3330, train_loss = 0.9476, dev_mse = 0.9095\n",
            "epoch = 3340, train_loss = 0.9469, dev_mse = 0.9090\n",
            "epoch = 3350, train_loss = 0.9462, dev_mse = 0.9078\n",
            "epoch = 3360, train_loss = 0.9455, dev_mse = 0.9071\n",
            "epoch = 3370, train_loss = 0.9448, dev_mse = 0.9063\n",
            "epoch = 3380, train_loss = 0.9441, dev_mse = 0.9056\n",
            "epoch = 3390, train_loss = 0.9434, dev_mse = 0.9047\n",
            "epoch = 3400, train_loss = 0.9428, dev_mse = 0.9039\n",
            "epoch = 3410, train_loss = 0.9421, dev_mse = 0.9030\n",
            "epoch = 3420, train_loss = 0.9414, dev_mse = 0.9024\n",
            "epoch = 3430, train_loss = 0.9408, dev_mse = 0.9015\n",
            "epoch = 3440, train_loss = 0.9401, dev_mse = 0.9008\n",
            "epoch = 3450, train_loss = 0.9394, dev_mse = 0.9001\n",
            "epoch = 3460, train_loss = 0.9388, dev_mse = 0.8990\n",
            "epoch = 3470, train_loss = 0.9381, dev_mse = 0.8983\n",
            "epoch = 3480, train_loss = 0.9375, dev_mse = 0.8978\n",
            "epoch = 3490, train_loss = 0.9368, dev_mse = 0.8968\n",
            "epoch = 3500, train_loss = 0.9362, dev_mse = 0.8961\n",
            "epoch = 3510, train_loss = 0.9356, dev_mse = 0.8955\n",
            "epoch = 3520, train_loss = 0.9350, dev_mse = 0.8945\n",
            "epoch = 3530, train_loss = 0.9343, dev_mse = 0.8941\n",
            "epoch = 3540, train_loss = 0.9337, dev_mse = 0.8929\n",
            "epoch = 3550, train_loss = 0.9331, dev_mse = 0.8924\n",
            "epoch = 3560, train_loss = 0.9325, dev_mse = 0.8917\n",
            "epoch = 3570, train_loss = 0.9319, dev_mse = 0.8911\n",
            "epoch = 3580, train_loss = 0.9313, dev_mse = 0.8901\n",
            "epoch = 3590, train_loss = 0.9307, dev_mse = 0.8894\n",
            "epoch = 3600, train_loss = 0.9301, dev_mse = 0.8887\n",
            "epoch = 3610, train_loss = 0.9295, dev_mse = 0.8879\n",
            "epoch = 3620, train_loss = 0.9289, dev_mse = 0.8873\n",
            "epoch = 3630, train_loss = 0.9283, dev_mse = 0.8867\n",
            "epoch = 3640, train_loss = 0.9277, dev_mse = 0.8859\n",
            "epoch = 3650, train_loss = 0.9272, dev_mse = 0.8852\n",
            "epoch = 3660, train_loss = 0.9266, dev_mse = 0.8843\n",
            "epoch = 3670, train_loss = 0.9260, dev_mse = 0.8840\n",
            "epoch = 3680, train_loss = 0.9254, dev_mse = 0.8832\n",
            "epoch = 3690, train_loss = 0.9248, dev_mse = 0.8825\n",
            "epoch = 3700, train_loss = 0.9243, dev_mse = 0.8816\n",
            "epoch = 3710, train_loss = 0.9237, dev_mse = 0.8810\n",
            "epoch = 3720, train_loss = 0.9232, dev_mse = 0.8799\n",
            "epoch = 3730, train_loss = 0.9226, dev_mse = 0.8797\n",
            "epoch = 3740, train_loss = 0.9221, dev_mse = 0.8787\n",
            "epoch = 3750, train_loss = 0.9215, dev_mse = 0.8781\n",
            "epoch = 3760, train_loss = 0.9210, dev_mse = 0.8770\n",
            "epoch = 3770, train_loss = 0.9205, dev_mse = 0.8772\n",
            "epoch = 3780, train_loss = 0.9200, dev_mse = 0.8756\n",
            "epoch = 3790, train_loss = 0.9194, dev_mse = 0.8760\n",
            "epoch = 3800, train_loss = 0.9189, dev_mse = 0.8744\n",
            "epoch = 3810, train_loss = 0.9184, dev_mse = 0.8745\n",
            "epoch = 3820, train_loss = 0.9180, dev_mse = 0.8750\n",
            "epoch = 3830, train_loss = 0.9174, dev_mse = 0.8731\n",
            "epoch = 3840, train_loss = 0.9170, dev_mse = 0.8737\n",
            "epoch = 3850, train_loss = 0.9164, dev_mse = 0.8705\n",
            "epoch = 3860, train_loss = 0.9159, dev_mse = 0.8708\n",
            "epoch = 3870, train_loss = 0.9155, dev_mse = 0.8720\n",
            "epoch = 3880, train_loss = 0.9150, dev_mse = 0.8684\n",
            "epoch = 3890, train_loss = 0.9145, dev_mse = 0.8694\n",
            "epoch = 3900, train_loss = 0.9141, dev_mse = 0.8685\n",
            "epoch = 3910, train_loss = 0.9136, dev_mse = 0.8674\n",
            "epoch = 3920, train_loss = 0.9132, dev_mse = 0.8661\n",
            "epoch = 3930, train_loss = 0.9127, dev_mse = 0.8677\n",
            "epoch = 3940, train_loss = 0.9123, dev_mse = 0.8673\n",
            "epoch = 3950, train_loss = 0.9118, dev_mse = 0.8660\n",
            "epoch = 3960, train_loss = 0.9114, dev_mse = 0.8656\n",
            "epoch = 3970, train_loss = 0.9110, dev_mse = 0.8654\n",
            "epoch = 3980, train_loss = 0.9106, dev_mse = 0.8638\n",
            "epoch = 3990, train_loss = 0.9102, dev_mse = 0.8644\n",
            "epoch = 4000, train_loss = 0.9098, dev_mse = 0.8620\n",
            "epoch = 4010, train_loss = 0.9094, dev_mse = 0.8637\n",
            "epoch = 4020, train_loss = 0.9090, dev_mse = 0.8612\n",
            "epoch = 4030, train_loss = 0.9086, dev_mse = 0.8623\n",
            "epoch = 4040, train_loss = 0.9082, dev_mse = 0.8603\n",
            "epoch = 4050, train_loss = 0.9078, dev_mse = 0.8611\n",
            "epoch = 4060, train_loss = 0.9074, dev_mse = 0.8594\n",
            "epoch = 4070, train_loss = 0.9071, dev_mse = 0.8599\n",
            "epoch = 4080, train_loss = 0.9067, dev_mse = 0.8594\n",
            "epoch = 4090, train_loss = 0.9063, dev_mse = 0.8587\n",
            "epoch = 4100, train_loss = 0.9061, dev_mse = 0.8597\n",
            "epoch = 4110, train_loss = 0.9056, dev_mse = 0.8570\n",
            "epoch = 4120, train_loss = 0.9053, dev_mse = 0.8577\n",
            "epoch = 4130, train_loss = 0.9049, dev_mse = 0.8564\n",
            "epoch = 4140, train_loss = 0.9046, dev_mse = 0.8565\n",
            "epoch = 4150, train_loss = 0.9042, dev_mse = 0.8559\n",
            "epoch = 4160, train_loss = 0.9040, dev_mse = 0.8543\n",
            "epoch = 4170, train_loss = 0.9037, dev_mse = 0.8536\n",
            "epoch = 4180, train_loss = 0.9034, dev_mse = 0.8563\n",
            "epoch = 4190, train_loss = 0.9029, dev_mse = 0.8536\n",
            "epoch = 4200, train_loss = 0.9026, dev_mse = 0.8534\n",
            "epoch = 4210, train_loss = 0.9023, dev_mse = 0.8533\n",
            "epoch = 4220, train_loss = 0.9020, dev_mse = 0.8543\n",
            "epoch = 4230, train_loss = 0.9017, dev_mse = 0.8526\n",
            "epoch = 4240, train_loss = 0.9015, dev_mse = 0.8505\n",
            "epoch = 4250, train_loss = 0.9011, dev_mse = 0.8526\n",
            "epoch = 4260, train_loss = 0.9008, dev_mse = 0.8508\n",
            "epoch = 4270, train_loss = 0.9005, dev_mse = 0.8498\n",
            "epoch = 4280, train_loss = 0.9002, dev_mse = 0.8509\n",
            "epoch = 4290, train_loss = 0.8999, dev_mse = 0.8510\n",
            "epoch = 4300, train_loss = 0.8997, dev_mse = 0.8505\n",
            "epoch = 4310, train_loss = 0.8994, dev_mse = 0.8489\n",
            "epoch = 4320, train_loss = 0.8994, dev_mse = 0.8465\n",
            "epoch = 4330, train_loss = 0.8989, dev_mse = 0.8505\n",
            "epoch = 4340, train_loss = 0.8986, dev_mse = 0.8472\n",
            "epoch = 4350, train_loss = 0.8983, dev_mse = 0.8484\n",
            "epoch = 4360, train_loss = 0.8981, dev_mse = 0.8473\n",
            "epoch = 4370, train_loss = 0.8978, dev_mse = 0.8474\n",
            "epoch = 4380, train_loss = 0.8975, dev_mse = 0.8464\n",
            "epoch = 4390, train_loss = 0.8973, dev_mse = 0.8463\n",
            "epoch = 4400, train_loss = 0.8971, dev_mse = 0.8470\n",
            "epoch = 4410, train_loss = 0.8971, dev_mse = 0.8481\n",
            "epoch = 4420, train_loss = 0.8967, dev_mse = 0.8445\n",
            "epoch = 4430, train_loss = 0.8964, dev_mse = 0.8464\n",
            "epoch = 4440, train_loss = 0.8962, dev_mse = 0.8444\n",
            "epoch = 4450, train_loss = 0.8959, dev_mse = 0.8451\n",
            "epoch = 4460, train_loss = 0.8957, dev_mse = 0.8443\n",
            "epoch = 4470, train_loss = 0.8955, dev_mse = 0.8439\n",
            "epoch = 4480, train_loss = 0.8953, dev_mse = 0.8438\n",
            "epoch = 4490, train_loss = 0.8951, dev_mse = 0.8436\n",
            "epoch = 4500, train_loss = 0.8949, dev_mse = 0.8433\n",
            "epoch = 4510, train_loss = 0.8947, dev_mse = 0.8429\n",
            "epoch = 4520, train_loss = 0.8949, dev_mse = 0.8396\n",
            "epoch = 4530, train_loss = 0.8946, dev_mse = 0.8458\n",
            "epoch = 4540, train_loss = 0.8941, dev_mse = 0.8426\n",
            "epoch = 4550, train_loss = 0.8939, dev_mse = 0.8411\n",
            "epoch = 4560, train_loss = 0.8937, dev_mse = 0.8420\n",
            "epoch = 4570, train_loss = 0.8935, dev_mse = 0.8420\n",
            "epoch = 4580, train_loss = 0.8934, dev_mse = 0.8419\n",
            "epoch = 4590, train_loss = 0.8932, dev_mse = 0.8409\n",
            "epoch = 4600, train_loss = 0.8930, dev_mse = 0.8400\n",
            "epoch = 4610, train_loss = 0.8929, dev_mse = 0.8398\n",
            "epoch = 4620, train_loss = 0.8927, dev_mse = 0.8397\n",
            "epoch = 4630, train_loss = 0.8925, dev_mse = 0.8399\n",
            "epoch = 4640, train_loss = 0.8925, dev_mse = 0.8414\n",
            "epoch = 4650, train_loss = 0.8922, dev_mse = 0.8388\n",
            "epoch = 4660, train_loss = 0.8920, dev_mse = 0.8399\n",
            "epoch = 4670, train_loss = 0.8919, dev_mse = 0.8404\n",
            "epoch = 4680, train_loss = 0.8917, dev_mse = 0.8396\n",
            "epoch = 4690, train_loss = 0.8917, dev_mse = 0.8407\n",
            "epoch = 4700, train_loss = 0.8915, dev_mse = 0.8376\n",
            "epoch = 4710, train_loss = 0.8913, dev_mse = 0.8384\n",
            "epoch = 4720, train_loss = 0.8912, dev_mse = 0.8374\n",
            "epoch = 4730, train_loss = 0.8910, dev_mse = 0.8377\n",
            "epoch = 4740, train_loss = 0.8909, dev_mse = 0.8373\n",
            "epoch = 4750, train_loss = 0.8908, dev_mse = 0.8371\n",
            "epoch = 4760, train_loss = 0.8906, dev_mse = 0.8371\n",
            "epoch = 4770, train_loss = 0.8905, dev_mse = 0.8368\n",
            "epoch = 4780, train_loss = 0.8904, dev_mse = 0.8367\n",
            "epoch = 4790, train_loss = 0.8903, dev_mse = 0.8368\n",
            "epoch = 4800, train_loss = 0.8902, dev_mse = 0.8393\n",
            "epoch = 4810, train_loss = 0.8900, dev_mse = 0.8357\n",
            "epoch = 4820, train_loss = 0.8899, dev_mse = 0.8382\n",
            "epoch = 4830, train_loss = 0.8898, dev_mse = 0.8357\n",
            "epoch = 4840, train_loss = 0.8896, dev_mse = 0.8362\n",
            "epoch = 4850, train_loss = 0.8895, dev_mse = 0.8372\n",
            "epoch = 4860, train_loss = 0.8894, dev_mse = 0.8373\n",
            "epoch = 4870, train_loss = 0.8893, dev_mse = 0.8357\n",
            "epoch = 4880, train_loss = 0.8891, dev_mse = 0.8357\n",
            "epoch = 4890, train_loss = 0.8890, dev_mse = 0.8354\n",
            "epoch = 4900, train_loss = 0.8889, dev_mse = 0.8360\n",
            "epoch = 4910, train_loss = 0.8891, dev_mse = 0.8386\n",
            "epoch = 4920, train_loss = 0.8888, dev_mse = 0.8340\n",
            "epoch = 4930, train_loss = 0.8886, dev_mse = 0.8361\n",
            "epoch = 4940, train_loss = 0.8885, dev_mse = 0.8351\n",
            "epoch = 4950, train_loss = 0.8884, dev_mse = 0.8346\n",
            "epoch = 4960, train_loss = 0.8883, dev_mse = 0.8351\n",
            "epoch = 4970, train_loss = 0.8882, dev_mse = 0.8354\n",
            "epoch = 4980, train_loss = 0.8881, dev_mse = 0.8342\n",
            "epoch = 4990, train_loss = 0.8881, dev_mse = 0.8331\n",
            "epoch = 5000, train_loss = 0.8879, dev_mse = 0.8349\n",
            "epoch = 5010, train_loss = 0.8878, dev_mse = 0.8348\n",
            "epoch = 5020, train_loss = 0.8877, dev_mse = 0.8335\n",
            "epoch = 5030, train_loss = 0.8878, dev_mse = 0.8325\n",
            "epoch = 5040, train_loss = 0.8875, dev_mse = 0.8346\n",
            "epoch = 5050, train_loss = 0.8875, dev_mse = 0.8351\n",
            "epoch = 5060, train_loss = 0.8873, dev_mse = 0.8333\n",
            "epoch = 5070, train_loss = 0.8873, dev_mse = 0.8329\n",
            "epoch = 5080, train_loss = 0.8872, dev_mse = 0.8333\n",
            "epoch = 5090, train_loss = 0.8871, dev_mse = 0.8339\n",
            "epoch = 5100, train_loss = 0.8870, dev_mse = 0.8344\n",
            "epoch = 5110, train_loss = 0.8870, dev_mse = 0.8355\n",
            "epoch = 5120, train_loss = 0.8869, dev_mse = 0.8338\n",
            "epoch = 5130, train_loss = 0.8868, dev_mse = 0.8328\n",
            "epoch = 5140, train_loss = 0.8867, dev_mse = 0.8337\n",
            "epoch = 5150, train_loss = 0.8866, dev_mse = 0.8324\n",
            "epoch = 5160, train_loss = 0.8865, dev_mse = 0.8335\n",
            "epoch = 5170, train_loss = 0.8864, dev_mse = 0.8330\n",
            "epoch = 5180, train_loss = 0.8864, dev_mse = 0.8322\n",
            "epoch = 5190, train_loss = 0.8863, dev_mse = 0.8326\n",
            "epoch = 5200, train_loss = 0.8862, dev_mse = 0.8340\n",
            "epoch = 5210, train_loss = 0.8862, dev_mse = 0.8337\n",
            "epoch = 5220, train_loss = 0.8861, dev_mse = 0.8323\n",
            "epoch = 5230, train_loss = 0.8858, dev_mse = 0.8330\n",
            "epoch = 5240, train_loss = 0.8853, dev_mse = 0.8358\n",
            "epoch = 5250, train_loss = 0.8851, dev_mse = 0.8366\n",
            "epoch = 5260, train_loss = 0.8849, dev_mse = 0.8347\n",
            "epoch = 5270, train_loss = 0.8847, dev_mse = 0.8347\n",
            "epoch = 5280, train_loss = 0.8846, dev_mse = 0.8345\n",
            "epoch = 5290, train_loss = 0.8844, dev_mse = 0.8347\n",
            "epoch = 5300, train_loss = 0.8843, dev_mse = 0.8346\n",
            "epoch = 5310, train_loss = 0.8842, dev_mse = 0.8336\n",
            "epoch = 5320, train_loss = 0.8841, dev_mse = 0.8340\n",
            "epoch = 5330, train_loss = 0.8838, dev_mse = 0.8361\n",
            "epoch = 5340, train_loss = 0.8838, dev_mse = 0.8368\n",
            "epoch = 5350, train_loss = 0.8837, dev_mse = 0.8368\n",
            "epoch = 5360, train_loss = 0.8834, dev_mse = 0.8348\n",
            "epoch = 5370, train_loss = 0.8833, dev_mse = 0.8343\n",
            "epoch = 5380, train_loss = 0.8832, dev_mse = 0.8345\n",
            "epoch = 5390, train_loss = 0.8830, dev_mse = 0.8346\n",
            "epoch = 5400, train_loss = 0.8829, dev_mse = 0.8341\n",
            "epoch = 5410, train_loss = 0.8829, dev_mse = 0.8331\n",
            "epoch = 5420, train_loss = 0.8834, dev_mse = 0.8323\n",
            "epoch = 5430, train_loss = 0.8829, dev_mse = 0.8320\n",
            "epoch = 5440, train_loss = 0.8825, dev_mse = 0.8327\n",
            "epoch = 5450, train_loss = 0.8824, dev_mse = 0.8338\n",
            "epoch = 5460, train_loss = 0.8823, dev_mse = 0.8331\n",
            "epoch = 5470, train_loss = 0.8822, dev_mse = 0.8323\n",
            "epoch = 5480, train_loss = 0.8821, dev_mse = 0.8334\n",
            "epoch = 5490, train_loss = 0.8820, dev_mse = 0.8324\n",
            "epoch = 5500, train_loss = 0.8819, dev_mse = 0.8328\n",
            "epoch = 5510, train_loss = 0.8818, dev_mse = 0.8327\n",
            "epoch = 5520, train_loss = 0.8817, dev_mse = 0.8329\n",
            "epoch = 5530, train_loss = 0.8816, dev_mse = 0.8317\n",
            "epoch = 5540, train_loss = 0.8815, dev_mse = 0.8316\n",
            "epoch = 5550, train_loss = 0.8814, dev_mse = 0.8326\n",
            "epoch = 5560, train_loss = 0.8813, dev_mse = 0.8316\n",
            "epoch = 5570, train_loss = 0.8813, dev_mse = 0.8318\n",
            "epoch = 5580, train_loss = 0.8812, dev_mse = 0.8319\n",
            "epoch = 5590, train_loss = 0.8811, dev_mse = 0.8318\n",
            "epoch = 5600, train_loss = 0.8810, dev_mse = 0.8318\n",
            "epoch = 5610, train_loss = 0.8809, dev_mse = 0.8317\n",
            "epoch = 5620, train_loss = 0.8808, dev_mse = 0.8317\n",
            "epoch = 5630, train_loss = 0.8807, dev_mse = 0.8307\n",
            "epoch = 5640, train_loss = 0.8806, dev_mse = 0.8306\n",
            "epoch = 5650, train_loss = 0.8804, dev_mse = 0.8320\n",
            "epoch = 5660, train_loss = 0.8803, dev_mse = 0.8322\n",
            "epoch = 5670, train_loss = 0.8802, dev_mse = 0.8315\n",
            "epoch = 5680, train_loss = 0.8801, dev_mse = 0.8312\n",
            "epoch = 5690, train_loss = 0.8801, dev_mse = 0.8309\n",
            "epoch = 5700, train_loss = 0.8802, dev_mse = 0.8295\n",
            "epoch = 5710, train_loss = 0.8799, dev_mse = 0.8311\n",
            "epoch = 5720, train_loss = 0.8798, dev_mse = 0.8319\n",
            "epoch = 5730, train_loss = 0.8797, dev_mse = 0.8311\n",
            "epoch = 5740, train_loss = 0.8796, dev_mse = 0.8320\n",
            "epoch = 5750, train_loss = 0.8796, dev_mse = 0.8310\n",
            "epoch = 5760, train_loss = 0.8795, dev_mse = 0.8311\n",
            "epoch = 5770, train_loss = 0.8794, dev_mse = 0.8303\n",
            "epoch = 5780, train_loss = 0.8801, dev_mse = 0.8287\n",
            "epoch = 5790, train_loss = 0.8794, dev_mse = 0.8319\n",
            "epoch = 5800, train_loss = 0.8792, dev_mse = 0.8323\n",
            "epoch = 5810, train_loss = 0.8791, dev_mse = 0.8304\n",
            "epoch = 5820, train_loss = 0.8790, dev_mse = 0.8314\n",
            "epoch = 5830, train_loss = 0.8790, dev_mse = 0.8316\n",
            "epoch = 5840, train_loss = 0.8789, dev_mse = 0.8311\n",
            "epoch = 5850, train_loss = 0.8788, dev_mse = 0.8317\n",
            "epoch = 5860, train_loss = 0.8787, dev_mse = 0.8312\n",
            "epoch = 5870, train_loss = 0.8787, dev_mse = 0.8311\n",
            "epoch = 5880, train_loss = 0.8786, dev_mse = 0.8314\n",
            "epoch = 5890, train_loss = 0.8785, dev_mse = 0.8310\n",
            "epoch = 5900, train_loss = 0.8784, dev_mse = 0.8314\n",
            "epoch = 5910, train_loss = 0.8784, dev_mse = 0.8317\n",
            "epoch = 5920, train_loss = 0.8783, dev_mse = 0.8321\n",
            "epoch = 5930, train_loss = 0.8785, dev_mse = 0.8335\n",
            "epoch = 5940, train_loss = 0.8782, dev_mse = 0.8311\n",
            "epoch = 5950, train_loss = 0.8781, dev_mse = 0.8307\n",
            "epoch = 5960, train_loss = 0.8781, dev_mse = 0.8314\n",
            "epoch = 5970, train_loss = 0.8781, dev_mse = 0.8326\n",
            "epoch = 5980, train_loss = 0.8783, dev_mse = 0.8337\n",
            "epoch = 5990, train_loss = 0.8779, dev_mse = 0.8302\n",
            "epoch = 6000, train_loss = 0.8778, dev_mse = 0.8313\n",
            "epoch = 6010, train_loss = 0.8778, dev_mse = 0.8318\n",
            "epoch = 6020, train_loss = 0.8777, dev_mse = 0.8311\n",
            "epoch = 6030, train_loss = 0.8779, dev_mse = 0.8287\n",
            "epoch = 6040, train_loss = 0.8776, dev_mse = 0.8315\n",
            "epoch = 6050, train_loss = 0.8776, dev_mse = 0.8298\n",
            "epoch = 6060, train_loss = 0.8776, dev_mse = 0.8324\n",
            "epoch = 6070, train_loss = 0.8775, dev_mse = 0.8299\n",
            "epoch = 6080, train_loss = 0.8774, dev_mse = 0.8316\n",
            "epoch = 6090, train_loss = 0.8774, dev_mse = 0.8306\n",
            "epoch = 6100, train_loss = 0.8774, dev_mse = 0.8297\n",
            "epoch = 6110, train_loss = 0.8774, dev_mse = 0.8323\n",
            "epoch = 6120, train_loss = 0.8772, dev_mse = 0.8296\n",
            "epoch = 6130, train_loss = 0.8771, dev_mse = 0.8303\n",
            "epoch = 6140, train_loss = 0.8773, dev_mse = 0.8323\n",
            "epoch = 6150, train_loss = 0.8771, dev_mse = 0.8293\n",
            "epoch = 6160, train_loss = 0.8770, dev_mse = 0.8311\n",
            "epoch = 6170, train_loss = 0.8770, dev_mse = 0.8307\n",
            "epoch = 6180, train_loss = 0.8771, dev_mse = 0.8288\n",
            "epoch = 6190, train_loss = 0.8770, dev_mse = 0.8322\n",
            "epoch = 6200, train_loss = 0.8768, dev_mse = 0.8290\n",
            "epoch = 6210, train_loss = 0.8768, dev_mse = 0.8301\n",
            "epoch = 6220, train_loss = 0.8769, dev_mse = 0.8319\n",
            "epoch = 6230, train_loss = 0.8769, dev_mse = 0.8280\n",
            "epoch = 6240, train_loss = 0.8768, dev_mse = 0.8318\n",
            "epoch = 6250, train_loss = 0.8766, dev_mse = 0.8293\n",
            "epoch = 6260, train_loss = 0.8766, dev_mse = 0.8293\n",
            "epoch = 6270, train_loss = 0.8767, dev_mse = 0.8317\n",
            "epoch = 6280, train_loss = 0.8765, dev_mse = 0.8286\n",
            "epoch = 6290, train_loss = 0.8764, dev_mse = 0.8304\n",
            "epoch = 6300, train_loss = 0.8763, dev_mse = 0.8296\n",
            "epoch = 6310, train_loss = 0.8764, dev_mse = 0.8285\n",
            "epoch = 6320, train_loss = 0.8763, dev_mse = 0.8292\n",
            "epoch = 6330, train_loss = 0.8762, dev_mse = 0.8305\n",
            "epoch = 6340, train_loss = 0.8763, dev_mse = 0.8310\n",
            "epoch = 6350, train_loss = 0.8762, dev_mse = 0.8302\n",
            "epoch = 6360, train_loss = 0.8761, dev_mse = 0.8288\n",
            "epoch = 6370, train_loss = 0.8762, dev_mse = 0.8277\n",
            "epoch = 6380, train_loss = 0.8760, dev_mse = 0.8298\n",
            "epoch = 6390, train_loss = 0.8760, dev_mse = 0.8304\n",
            "epoch = 6400, train_loss = 0.8759, dev_mse = 0.8298\n",
            "epoch = 6410, train_loss = 0.8759, dev_mse = 0.8284\n",
            "epoch = 6420, train_loss = 0.8761, dev_mse = 0.8271\n",
            "epoch = 6430, train_loss = 0.8758, dev_mse = 0.8286\n",
            "epoch = 6440, train_loss = 0.8758, dev_mse = 0.8303\n",
            "epoch = 6450, train_loss = 0.8758, dev_mse = 0.8304\n",
            "epoch = 6460, train_loss = 0.8757, dev_mse = 0.8289\n",
            "epoch = 6470, train_loss = 0.8757, dev_mse = 0.8279\n",
            "epoch = 6480, train_loss = 0.8758, dev_mse = 0.8271\n",
            "epoch = 6490, train_loss = 0.8756, dev_mse = 0.8287\n",
            "epoch = 6500, train_loss = 0.8756, dev_mse = 0.8306\n",
            "epoch = 6510, train_loss = 0.8755, dev_mse = 0.8297\n",
            "epoch = 6520, train_loss = 0.8755, dev_mse = 0.8283\n",
            "epoch = 6530, train_loss = 0.8756, dev_mse = 0.8272\n",
            "epoch = 6540, train_loss = 0.8754, dev_mse = 0.8284\n",
            "epoch = 6550, train_loss = 0.8754, dev_mse = 0.8299\n",
            "epoch = 6560, train_loss = 0.8755, dev_mse = 0.8310\n",
            "epoch = 6570, train_loss = 0.8753, dev_mse = 0.8286\n",
            "epoch = 6580, train_loss = 0.8754, dev_mse = 0.8270\n",
            "epoch = 6590, train_loss = 0.8752, dev_mse = 0.8284\n",
            "epoch = 6600, train_loss = 0.8752, dev_mse = 0.8297\n",
            "epoch = 6610, train_loss = 0.8753, dev_mse = 0.8307\n",
            "epoch = 6620, train_loss = 0.8751, dev_mse = 0.8286\n",
            "epoch = 6630, train_loss = 0.8751, dev_mse = 0.8276\n",
            "epoch = 6640, train_loss = 0.8752, dev_mse = 0.8270\n",
            "epoch = 6650, train_loss = 0.8750, dev_mse = 0.8292\n",
            "epoch = 6660, train_loss = 0.8752, dev_mse = 0.8312\n",
            "epoch = 6670, train_loss = 0.8749, dev_mse = 0.8285\n",
            "epoch = 6680, train_loss = 0.8749, dev_mse = 0.8274\n",
            "epoch = 6690, train_loss = 0.8749, dev_mse = 0.8274\n",
            "epoch = 6700, train_loss = 0.8748, dev_mse = 0.8288\n",
            "epoch = 6710, train_loss = 0.8748, dev_mse = 0.8300\n",
            "epoch = 6720, train_loss = 0.8749, dev_mse = 0.8303\n",
            "epoch = 6730, train_loss = 0.8747, dev_mse = 0.8280\n",
            "epoch = 6740, train_loss = 0.8749, dev_mse = 0.8265\n",
            "epoch = 6750, train_loss = 0.8747, dev_mse = 0.8296\n",
            "epoch = 6760, train_loss = 0.8746, dev_mse = 0.8294\n",
            "epoch = 6770, train_loss = 0.8748, dev_mse = 0.8263\n",
            "epoch = 6780, train_loss = 0.8745, dev_mse = 0.8287\n",
            "epoch = 6790, train_loss = 0.8746, dev_mse = 0.8299\n",
            "epoch = 6800, train_loss = 0.8745, dev_mse = 0.8275\n",
            "epoch = 6810, train_loss = 0.8745, dev_mse = 0.8274\n",
            "epoch = 6820, train_loss = 0.8744, dev_mse = 0.8295\n",
            "epoch = 6830, train_loss = 0.8745, dev_mse = 0.8300\n",
            "epoch = 6840, train_loss = 0.8744, dev_mse = 0.8298\n",
            "epoch = 6850, train_loss = 0.8745, dev_mse = 0.8310\n",
            "epoch = 6860, train_loss = 0.8742, dev_mse = 0.8279\n",
            "epoch = 6870, train_loss = 0.8745, dev_mse = 0.8263\n",
            "epoch = 6880, train_loss = 0.8744, dev_mse = 0.8311\n",
            "epoch = 6890, train_loss = 0.8742, dev_mse = 0.8273\n",
            "epoch = 6900, train_loss = 0.8741, dev_mse = 0.8276\n",
            "epoch = 6910, train_loss = 0.8741, dev_mse = 0.8286\n",
            "epoch = 6920, train_loss = 0.8741, dev_mse = 0.8291\n",
            "epoch = 6930, train_loss = 0.8741, dev_mse = 0.8304\n",
            "epoch = 6940, train_loss = 0.8741, dev_mse = 0.8295\n",
            "epoch = 6950, train_loss = 0.8740, dev_mse = 0.8265\n",
            "epoch = 6960, train_loss = 0.8740, dev_mse = 0.8267\n",
            "epoch = 6970, train_loss = 0.8739, dev_mse = 0.8298\n",
            "epoch = 6980, train_loss = 0.8738, dev_mse = 0.8281\n",
            "epoch = 6990, train_loss = 0.8740, dev_mse = 0.8266\n",
            "epoch = 7000, train_loss = 0.8739, dev_mse = 0.8303\n",
            "epoch = 7010, train_loss = 0.8737, dev_mse = 0.8273\n",
            "epoch = 7020, train_loss = 0.8738, dev_mse = 0.8270\n",
            "epoch = 7030, train_loss = 0.8738, dev_mse = 0.8303\n",
            "epoch = 7040, train_loss = 0.8736, dev_mse = 0.8283\n",
            "epoch = 7050, train_loss = 0.8736, dev_mse = 0.8271\n",
            "epoch = 7060, train_loss = 0.8735, dev_mse = 0.8276\n",
            "epoch = 7070, train_loss = 0.8736, dev_mse = 0.8263\n",
            "epoch = 7080, train_loss = 0.8741, dev_mse = 0.8253\n",
            "epoch = 7090, train_loss = 0.8738, dev_mse = 0.8309\n",
            "epoch = 7100, train_loss = 0.8735, dev_mse = 0.8265\n",
            "epoch = 7110, train_loss = 0.8734, dev_mse = 0.8290\n",
            "epoch = 7120, train_loss = 0.8733, dev_mse = 0.8276\n",
            "epoch = 7130, train_loss = 0.8734, dev_mse = 0.8266\n",
            "epoch = 7140, train_loss = 0.8734, dev_mse = 0.8295\n",
            "epoch = 7150, train_loss = 0.8733, dev_mse = 0.8286\n",
            "epoch = 7160, train_loss = 0.8733, dev_mse = 0.8261\n",
            "epoch = 7170, train_loss = 0.8732, dev_mse = 0.8269\n",
            "epoch = 7180, train_loss = 0.8733, dev_mse = 0.8299\n",
            "epoch = 7190, train_loss = 0.8731, dev_mse = 0.8265\n",
            "epoch = 7200, train_loss = 0.8732, dev_mse = 0.8259\n",
            "epoch = 7210, train_loss = 0.8730, dev_mse = 0.8284\n",
            "epoch = 7220, train_loss = 0.8730, dev_mse = 0.8284\n",
            "epoch = 7230, train_loss = 0.8729, dev_mse = 0.8275\n",
            "epoch = 7240, train_loss = 0.8730, dev_mse = 0.8261\n",
            "epoch = 7250, train_loss = 0.8735, dev_mse = 0.8252\n",
            "epoch = 7260, train_loss = 0.8735, dev_mse = 0.8306\n",
            "epoch = 7270, train_loss = 0.8731, dev_mse = 0.8260\n",
            "epoch = 7280, train_loss = 0.8728, dev_mse = 0.8284\n",
            "epoch = 7290, train_loss = 0.8728, dev_mse = 0.8262\n",
            "epoch = 7300, train_loss = 0.8727, dev_mse = 0.8281\n",
            "epoch = 7310, train_loss = 0.8727, dev_mse = 0.8286\n",
            "epoch = 7320, train_loss = 0.8726, dev_mse = 0.8284\n",
            "epoch = 7330, train_loss = 0.8726, dev_mse = 0.8277\n",
            "epoch = 7340, train_loss = 0.8726, dev_mse = 0.8293\n",
            "epoch = 7350, train_loss = 0.8728, dev_mse = 0.8294\n",
            "epoch = 7360, train_loss = 0.8729, dev_mse = 0.8247\n",
            "epoch = 7370, train_loss = 0.8725, dev_mse = 0.8289\n",
            "epoch = 7380, train_loss = 0.8725, dev_mse = 0.8279\n",
            "epoch = 7390, train_loss = 0.8726, dev_mse = 0.8253\n",
            "epoch = 7400, train_loss = 0.8723, dev_mse = 0.8271\n",
            "epoch = 7410, train_loss = 0.8723, dev_mse = 0.8287\n",
            "epoch = 7420, train_loss = 0.8725, dev_mse = 0.8291\n",
            "epoch = 7430, train_loss = 0.8728, dev_mse = 0.8245\n",
            "epoch = 7440, train_loss = 0.8723, dev_mse = 0.8289\n",
            "epoch = 7450, train_loss = 0.8722, dev_mse = 0.8282\n",
            "epoch = 7460, train_loss = 0.8724, dev_mse = 0.8250\n",
            "epoch = 7470, train_loss = 0.8725, dev_mse = 0.8310\n",
            "epoch = 7480, train_loss = 0.8721, dev_mse = 0.8261\n",
            "epoch = 7490, train_loss = 0.8721, dev_mse = 0.8264\n",
            "epoch = 7500, train_loss = 0.8722, dev_mse = 0.8293\n",
            "epoch = 7510, train_loss = 0.8723, dev_mse = 0.8245\n",
            "epoch = 7520, train_loss = 0.8723, dev_mse = 0.8300\n",
            "epoch = 7530, train_loss = 0.8721, dev_mse = 0.8248\n",
            "epoch = 7540, train_loss = 0.8719, dev_mse = 0.8284\n",
            "epoch = 7550, train_loss = 0.8719, dev_mse = 0.8274\n",
            "epoch = 7560, train_loss = 0.8721, dev_mse = 0.8250\n",
            "epoch = 7570, train_loss = 0.8720, dev_mse = 0.8296\n",
            "epoch = 7580, train_loss = 0.8718, dev_mse = 0.8254\n",
            "epoch = 7590, train_loss = 0.8717, dev_mse = 0.8269\n",
            "epoch = 7600, train_loss = 0.8718, dev_mse = 0.8283\n",
            "epoch = 7610, train_loss = 0.8718, dev_mse = 0.8251\n",
            "epoch = 7620, train_loss = 0.8716, dev_mse = 0.8272\n",
            "epoch = 7630, train_loss = 0.8718, dev_mse = 0.8287\n",
            "epoch = 7640, train_loss = 0.8721, dev_mse = 0.8241\n",
            "epoch = 7650, train_loss = 0.8717, dev_mse = 0.8285\n",
            "epoch = 7660, train_loss = 0.8714, dev_mse = 0.8262\n",
            "epoch = 7670, train_loss = 0.8714, dev_mse = 0.8260\n",
            "epoch = 7680, train_loss = 0.8715, dev_mse = 0.8249\n",
            "epoch = 7690, train_loss = 0.8713, dev_mse = 0.8261\n",
            "epoch = 7700, train_loss = 0.8717, dev_mse = 0.8246\n",
            "epoch = 7710, train_loss = 0.8713, dev_mse = 0.8260\n",
            "epoch = 7720, train_loss = 0.8719, dev_mse = 0.8238\n",
            "epoch = 7730, train_loss = 0.8714, dev_mse = 0.8293\n",
            "epoch = 7740, train_loss = 0.8713, dev_mse = 0.8244\n",
            "epoch = 7750, train_loss = 0.8711, dev_mse = 0.8270\n",
            "epoch = 7760, train_loss = 0.8714, dev_mse = 0.8288\n",
            "epoch = 7770, train_loss = 0.8712, dev_mse = 0.8248\n",
            "epoch = 7780, train_loss = 0.8710, dev_mse = 0.8274\n",
            "epoch = 7790, train_loss = 0.8710, dev_mse = 0.8264\n",
            "epoch = 7800, train_loss = 0.8709, dev_mse = 0.8258\n",
            "epoch = 7810, train_loss = 0.8712, dev_mse = 0.8238\n",
            "epoch = 7820, train_loss = 0.8709, dev_mse = 0.8259\n",
            "epoch = 7830, train_loss = 0.8715, dev_mse = 0.8299\n",
            "epoch = 7840, train_loss = 0.8714, dev_mse = 0.8231\n",
            "epoch = 7850, train_loss = 0.8709, dev_mse = 0.8279\n",
            "epoch = 7860, train_loss = 0.8708, dev_mse = 0.8272\n",
            "epoch = 7870, train_loss = 0.8711, dev_mse = 0.8237\n",
            "epoch = 7880, train_loss = 0.8709, dev_mse = 0.8289\n",
            "epoch = 7890, train_loss = 0.8706, dev_mse = 0.8254\n",
            "epoch = 7900, train_loss = 0.8707, dev_mse = 0.8247\n",
            "epoch = 7910, train_loss = 0.8706, dev_mse = 0.8250\n",
            "epoch = 7920, train_loss = 0.8706, dev_mse = 0.8247\n",
            "epoch = 7930, train_loss = 0.8709, dev_mse = 0.8234\n",
            "epoch = 7940, train_loss = 0.8705, dev_mse = 0.8278\n",
            "epoch = 7950, train_loss = 0.8705, dev_mse = 0.8258\n",
            "epoch = 7960, train_loss = 0.8707, dev_mse = 0.8244\n",
            "epoch = 7970, train_loss = 0.8706, dev_mse = 0.8280\n",
            "epoch = 7980, train_loss = 0.8705, dev_mse = 0.8237\n",
            "epoch = 7990, train_loss = 0.8703, dev_mse = 0.8269\n",
            "epoch = 8000, train_loss = 0.8704, dev_mse = 0.8265\n",
            "Finished training after 8000 epochs. min_mse: 0.8221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Gk3m4O05q23C",
        "outputId": "ba793319-e791-406b-fc53-46b9026e5813"
      },
      "source": [
        "plot_learning_curve(model_loss_record, title='deep model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xcZZ3n8c+vLn2tvqe7c+ncUOSWkRACwnAZwBsgi4o66KyzOuhkXXZHdNcZEXZnmYsO67izI+MqOgO6jlzUUUQZxBugAoKTQIBAAiEhkE7S6aQ7fa/u6qr67R/ndFJpujvdSVdXV+X7fr3q1afOec45v7r0r57znOc8x9wdEREpPZFCByAiIvmhBC8iUqKU4EVESpQSvIhIiVKCFxEpUUrwIiIlSgleZoWZXWBmLxQ6jvnCzM4zs61mNmBm75pG+W+Y2V/PRWxzxcweNrOPTrOsm9nr8x3T8UYJvgSY2Q4ze0shY3D3X7v7SYWMYZ75S+BL7p5w9x8UOhg5PinBy7SYWbTQMRyrOX4Ny4Hn5nB/Iq+hBF/CzCxiZteb2TYz6zKz75hZY87y75pZh5n1mtmvzOy0nGXfMLOvmNn9ZjYIXBweKXzKzJ4J1/m2mVWE5S8ys/ac9SctGy7/MzPbY2a7zeyjUx2im1mjmX09LHvAzH4Qzv+wmT0yruzB7UzwGj4Vvt5oTvl3m9kz03m/Jojrj83sJTPrNrMfmtnicP424ATgR2ETTfkE655hZk+aWb+ZfRuoGLf8CjPbaGY9ZvaYmb0xZ9liM/ueme0zs5fN7OM5y24ys38J3+/+cB+nT/Ea3MyuDZuT+s3sr8zsdeE++8L3oOxIrzlc9lYz2xJ+3l8CbNy+rjGzzeFn+BMzWz5ZXDJL3F2PIn8AO4C3TDD/OuBxoA0oB74K3JWz/BqgJlz298DGnGXfAHqB8wgqAhXhfn4LLAYagc3Ax8LyFwHt42KarOylQAdwGlAFfAtw4PWTvL5/Bb4NNABx4PfC+R8GHhlX9uB2JnkN24C35pT/LnD9dN6vcfu5BNgPrAnL/gPwqyN9JuGyMuAV4JPh63kvMAr8dbj8DKATeBMQBT4Ubq88fB0bgD8Pt3MCsB14e7juTeG23htu+1PAy0B8klgcuBeoDT+PEeAX4XbrgOeBDx3pNQMLgP6c/X4SSAMfDZe/E3gJOAWIAf8deGyiz02PWcwNhQ5Aj1n4ECdP8JuBN+c8XxT+88cmKFsf/pPVhc+/AXxzgv18MOf554Fbw+mLeG2Cn6zs7cDf5Cx7/WT/4GHMWaBhgmUf5sgJfvxr+Gvg9nC6BhgElh/F+3Ub8Pmc54mw7IqpPpNw2YXAbsBy5j3GoQT/FeCvxq3zAvB7BEn/1XHLPgN8PZy+CXg8Z1kE2ANcMEksDpyX83wD8Omc5/8b+PsjvWbgP4zbrwHtHErwPwY+Mi6uoZz3Xgk+Dw810ZS25cA94WF+D0ECywCtZhY1s5vD5og+goQEQU1szM4JttmRMz1E8E8+mcnKLh637Yn2M2Yp0O3uB6YoM5Xx274TuCpsNrkKeNLdXwmXTfp+TbDdxQS1cADcfQDoApZMI6bFwC4PM1volZzp5cB/G4sjjGVpuN5yYPG4ZTeMi/Hga3b3LEGiXczk9uZMJyd4nvu5TfaaD/tMw9eW+94vB76YE3M3wY/AdN4vOUqxQgcgebUTuMbdHx2/wMz+kOCw+S0Eyb0OOMDh7ab5Gmp0D0EzyJilU5TdCTSaWb2794xbNkjQxAOAmS2cYP3DXoO7P29mrwCXAX9AkPBz9zXh+zWB3QRJa2zf1UATsGsa6+4BlpiZ5ST5ZQTNR2NxfNbdPzt+RTM7F3jZ3U+cYvtLc8pHCN7r3dOI60imes17xu3XOPxzHXtNd8xCHDJNqsGXjriZVeQ8YsCtwGfHTmaZWbOZvTMsX0PQ3tpFkCQ/N4exfgf4IzM7xcyqgP8xWUF330NweP9lM2sws7iZXRgufho4zcxWhydwb5rm/u8kaG+/kKANfsxU79d4d4WvYXV4NPA54Al33zGN/f+GoH364+HruQo4O2f5PwIfM7M3WaDazN5hZjUE5zX6zezTZlYZHomtMrOzctY/08yuCr8DnyD4nB+fRlxHMtVr/leCz2Jsvx8Hcn9wbwU+Y+GJfDOrM7P3zUJMMgUl+NJxP8Hh9NjjJuCLwA+Bn5pZP8E/+ZvC8t8kONzeRXAibTYSwLS4+4+BW4CHCE68je17ZJJV/pCgrXcLwcnHT4TbeZGgv/nPga3AI5OsP95dBO3ZD7r7/pz5U71f41/Dzwl+mL5HUHt9HfD+6ezc3VMEzUMfJmiquBr4fs7y9cAfA18iOKp6KSyLu2eAK4DVBCdP9wP/RHAENubecJsHCN67q9x9dDqxHSHuSV9z+D6+D7iZoNJwIvBozrr3AP8LuDtsEtxEcBQleWSHNwOKzD0zO4XgH77c3dOFjqeYmdlNBCcrP1joWKTwVIOXgrCg/3m5mTUQ1Ox+pOQuMrvymuAtuNjl2fCCjfX53JcUnf9I0NyyjaCnyn8qbDgipSevTTRmtgNYO66dU0RE5oCaaERESlS+a/AvE5zJd+Cr7v61CcqsA9YBVFdXn3nyySfnLZ6JpLu72TM0zIG6et5YU3XkFURE5pENGzbsd/fmiZblO8EvcfddZtYC/Az4E3f/1WTl165d6+vXz21Tfc8PfsDfPvw4t73z/ey48I1URHVQIyLFw8w2uPvaiZblNZu5+67wbydwD4dfzDEvRKqrqRoeBmAwky1wNCIisydvCT68+q5mbBp4G0Ff53klmkhQNZwEYCCTKXA0IiKzJ59j0bQSDNw0tp873f2BPO7vqESqq6kcUQ1eREpP3hK8u28HJr3RwHwRqa6mUk00IkVrdHSU9vZ2hsP/41JVUVFBW1sb8Xh82usc96NJRhIJqkbCJpq0mmhEik17ezs1NTWsWLGCsMWg5Lg7XV1dtLe3s3Llymmvd9x3Gck9yTqgGrxI0RkeHqapqalkkzuAmdHU1DTjoxQl+KqqnDZ41eBFilEpJ/cxR/Maj/sEb5EI1ZHgjVMNXkRKyXGf4AES0SgAg2kleBGZmZ6eHr785S/PeL3LL7+cnp7xNymbXUrwQHllBdFsVk00IjJjkyX4dHrq0a/vv/9+6uvr8xUWoF40QHCxU/VoSk00IjJj119/Pdu2bWP16tXE43EqKipoaGhgy5YtvPjii7zrXe9i586dDA8Pc91117Fu3ToAVqxYwfr16xkYGOCyyy7j/PPP57HHHmPJkiXce++9VFZWHnNsSvBAJFFNVWpEV7KKFLmOz32Okc1bZnWb5aeczMIbbph0+c0338ymTZvYuHEjDz/8MO94xzvYtGnTwe6Mt99+O42NjSSTSc466yze85730NTUdNg2tm7dyl133cU//uM/8vu///t873vf44MfPPabcinBA9FEDZUjw7rQSUSO2dlnn31YX/VbbrmFe+65B4CdO3eydevW1yT4lStXsnr1agDOPPNMduzYMSuxKMEDkZoaKoeTOskqUuSmqmnPlerq6oPTDz/8MD//+c/5zW9+Q1VVFRdddNGEfdnLy8sPTkejUZLJ5KzEopOsQLQmQeXQkJpoRGTGampq6O/vn3BZb28vDQ0NVFVVsWXLFh5//PE5jU01eCBSnaBqcID9aqIRkRlqamrivPPOY9WqVVRWVtLa2npw2aWXXsqtt97KKaecwkknncQ555wzp7EpwRM00VR09DEwOnW3JhGRidx5550Tzi8vL+fHP/7xhMvG2tkXLFjApk2HRlL/1Kc+NWtxqYmGoImmamRYg42JSElRgiccUXI4yWA2f7cvFBGZa0rwQCRRQ9VwkhQwklU7vIiUBiV4giaa6uTYmPBK8CJSGpTgCU6y6r6sIlJqlOAJuklWhwm+XydaRaREKMET9qIZS/DqCy8ix+imm27iC1/4QqHDUIIHsMpKqlIpQDV4ESkdSvAEt8Kq0V2dROQYfPazn+UNb3gD559/Pi+88AIA27Zt49JLL+XMM8/kggsuYMuWLfT29rJ8+XKyYY+9wcFBli5dyujo6KzHpCtZQ4lY8Funi51Eitf/2NrOpoHZGahrzKpEJX91YtuUZTZs2MDdd9/Nxo0bSafTrFmzhjPPPJN169Zx6623cuKJJ/LEE09w7bXX8uCDD7J69Wp++ctfcvHFF3Pffffx9re/nXg8PqtxgxL8QTXhm6s2eBGZqV//+te8+93vpqqqCoArr7yS4eFhHnvsMd73vvcdLDcyMgLA1Vdfzbe//W0uvvhi7r77bq699tq8xKUEH6quKCeSzaoGL1LEjlTTnkvZbJb6+no2btz4mmVXXnklN9xwA93d3WzYsIFLLrkkLzGoDT4UTSSoTo3Qr37wIjJDF154IT/4wQ9IJpP09/fzox/9iKqqKlauXMl3v/tdANydp59+GoBEIsFZZ53FddddxxVXXEE0Gs1LXErwoUhNDVUjw/TrSlYRmaE1a9Zw9dVXc/rpp3PZZZdx1llnAXDHHXdw2223cfrpp3Paaadx7733Hlzn6quv5lvf+hZXX3113uJSE00oWpOgKqmbfojI0bnxxhu58cYbXzP/gQcemLD8e9/7XtzzO8ChavChSKKGqqFB9YMXkZKhBB+KJBJUJ4foT+mmHyJSGpTgQ9GaBJXDw/Trrk4iRSffTR3zwdG8RiX4UCRRQ/XwkHrRiBSZiooKurq6SjrJuztdXV1UVFTMaD2dZA1FahJUJZMMZEr3SyJSitra2mhvb2ffvn2FDiWvKioqaGubWT9/JfhQNBEMGTwIZN2JmBU6JBGZhng8zsqVKwsdxrykJppQ7k0/BjVcgYiUgLwneDOLmtlTZnZfvvd1LMZq8KAhg0WkNMxFDf46YPMc7OeYRGpqqErqph8iUjrymuDNrA14B/BP+dzPbIhUVx+swWvAMREpBfmuwf898GfApFViM1tnZuvNbH0hz4JbNEoiDFNdJUWkFOQtwZvZFUCnu2+Yqpy7f83d17r72ubm5nyFMy2JaPB2aMAxESkF+azBnwdcaWY7gLuBS8zsW3nc3zFLxIJeo6rBi0gpyFuCd/fPuHubu68A3g886O4fzNf+ZkNNLBiTWW3wIlIK1A8+R6KiHFATjYiUhjm5ktXdHwYenot9HYuyRDUVqZTGhBeRkqAafI5oIkHVSJIB9YMXkRKgBJ8jkqihOpnUlawiUhKU4HNEahLBXZ00JryIlAAl+BzRRDDgWH9qtNChiIgcMyX4HJGaGqqHk6rBi0hJUILPEa0Na/BqgxeREqAEnyNaW0vVcJKBrO7qJCLFTwk+R2QswfvxcRNfESltSvA5orW1VCeTpM0YVi1eRIqcEnyOSE3twdv26WpWESl2SvA5ItVVVKdGAOjTiVYRKXJK8DnMjNrwph99GnBMRIqcEvw4NQdv+qEavIgUNyX4ceqiwZjwvUrwIlLklODHqSsLRlBWG7yIFDsl+HHqyoObfqgGLyLFTgl+nER1FdFMRjV4ESl6SvDjxGqDAcd6NeCYiBQ5JfhxIrV1JIYG6dWQwSJS5JTgx4nW1lCdHKR3eKTQoYiIHBMl+HGitbUkhoZUgxeRoqcEP06kppZEclAnWUWk6CnBjxOtC2rwfRpNUkSKnBL8OJGaGhLJIfqU30WkyCnBjxOtC3rRDFmEtGrxIlLElODHidbUUD08BECfxoQXkSKmBD+OxePUpIOLnHSiVUSKmRL8BGotaJrReDQiUsyU4CdQGwnelr5RJXgRKV5K8BOojWtMeBEpfkrwE6gviwNqgxeR4qYEP4G68jJACV5EipsS/AQS1dVEslk10YhIUVOCn0C8pobq5BB9GhNeRIpY3hK8mVWY2W/N7Gkze87M/iJf+5pt0fp6qpND9CSHCx2KiMhRi+Vx2yPAJe4+YGZx4BEz+7G7P57Hfc6KaEM9iQNDGhNeRIpa3mrwHhgIn8bDR1EM7hKtr9ddnUSk6OW1Dd7Moma2EegEfubuT0xQZp2ZrTez9fv27ctnONMWra8nkRyiN50tdCgiIkctrwne3TPuvhpoA842s1UTlPmau69197XNzc35DGfaDtbgvSgOOEREJjSjBG9mETOrnelO3L0HeAi4dKbrFkK0vp66wX561clIRIrYETOYmd1pZrVmVg1sAp43sz+dxnrNZlYfTlcCbwW2HGvAcyGSSFCTTDIciZDMqJlGRIrTdKqop7p7H/Au4MfASuAPp7HeIuAhM3sG+DeCNvj7jjrSOWRm1HtwkdMB9YUXkSI1nW6S8bCb47uAL7n7qJkdsXHa3Z8BzjjWAAulPnyJB9IZFhc4FhGRozGdGvxXgR1ANfArM1sO9OUzqPmgIRqMKKkavIgUqyPW4N39FuCWnFmvmNnF+QtpfmgIR5Q8oDHhRaRITeck63XhSVYzs9vM7EngkjmIraAaK4MRJVWDF5FiNZ0mmmvCk6xvAxoITrDenNeo5oHGqipACV5Eitd0EryFfy8H/tndn8uZV7Kq62opT43QrQHHRKRITSfBbzCznxIk+J+YWQ1Q8p3Dow0N1A4M0D2ULHQoIiJHZTrdJD8CrAa2u/uQmTUBf5TfsAovWl9P7a5+uodThQ5FROSoTKcXTdbM2oA/MDOAX7r7j/IeWYFF6+up3dpBj9rgRaRITacXzc3AdcDz4ePjZva5fAdWaNH6emoH+jmgoQpEpEhNp4nmcmC1u2cBzOz/AU8BN+QzsEKL1tdTOzhAj5f8+WQRKVHTHS6xPme6Lh+BzDfRujpqB/vpjURxDRssIkVoOjX4vwGeMrOHCLpHXghcn9eo5gGLxajLpMmY0Z/JUhuLFjokEZEZmc5J1rvM7GHgrHDWp929I69RzRP1YevMgdG0EryIFJ1JE7yZrRk3qz38u9jMFrv7k/kLa35oiAdJvXs0w/LKAgcjIjJDU9Xg//cUy5zjYTya8nIAutVVUkSK0KQJ3t1LfsTII2mpChL8/pQSvIgUH910dArNiQQA+4dHChyJiMjMKcFPobaxnrJUir19A4UORURkxpTgpxBraqKhv5f9GnBMRIrQpAnezD6YM33euGX/JZ9BzRdjCX7fiAYcE5HiM1UN/r/mTP/DuGXX5CGWeSfW1ER9fx/707ptn4gUn6kSvE0yPdHzkhRtWkBDXy9dGo9GRIrQVAneJ5me6HlJilRX0ZAcojsS03g0IlJ0prrQ6WQze4agtv66cJrw+Ql5j2weMDOasmnSkQg96QwN8ekM3SMiMj9MlbFOmbMo5rGm8BhnfyqtBC8iRWWqK1lfyX0e3qrvQuBVd9+Q78DmiwWx4C3al0pzYnWBgxERmYGpukneZ2arwulFwCaC3jP/bGafmKP4Cq65sgyA/RqPRkSKzFQnWVe6+6Zw+o+An7n7vwPexHHSTRKgpbYGgP3J4QJHIiIyM1Ml+NGc6TcD9wO4ez9w3NyodMGCRiybZW9vf6FDERGZkanOGu40sz8hGAd+DfAAgJlVAvE5iG1eqGxtpb6nn44K9YUXkeIyVQ3+I8BpwIeBq929J5x/DvD1PMc1b8RaW1nQ08WepIYrEJHiMlUvmk7gYxPMfwh4KJ9BzSex1laaex5l78JFhQ5FRGRGprpl3w+nWtHdr5z9cOafaH09C/p6eS6ie7KKSHGZqg3+XGAncBfwBMfJ+DPjmRmtmRS9sTKSmSyVUY2wLCLFYapstRC4AVgFfBF4K7Df3X/p7r+ci+Dmi4XhT9ve1OjUBUVE5pFJE7y7Z9z9AXf/EMGJ1ZeAh6c7FryZLTWzh8zseTN7zsyum6WY59zCsuBAZ8+IEryIFI8pB1cxs3LgHcAHgBXALcA909x2Gvhv7v6kmdUAG8zsZ+7+/DHEWxCLE1UA7B5WTxoRKR5TnWT9JkHzzP3AX+Rc1Tot7r4H2BNO95vZZmAJUHQJfkl9LQC7e/tgYWOBoxERmZ6pavAfBAaB64CPmx08x2qAu3vtdHdiZiuAMwhO1o5ftg5YB7Bs2bLpbnJONSxaSOVwkt0HdIJVRIrHVG3wEXevCR+1OY+aGSb3BPA94BPu3jfBfr7m7mvdfW1zc/PRvYo8iy9dSvOBbvYMDBU6FBGRactrldTM4gTJ/Q53/34+95VP8SVttBzool0jSopIEclbgregTec2YLO7/12+9jMXoolqFvf30h45bobgEZESkM8a/HnAHwKXmNnG8HF5HveXV0uzo/SUldOfzhQ6FBGRacnbPejc/RFK6OrXpfFgqIJXh1OclqgscDQiIkembiHTtCLsC79jMFngSEREpkcJfppWNAf933fs6ypwJCIi05O3JppS09y2hETXADu61ZNGRIqDavDTVLbyBBbt36cmGhEpGkrw0xRraWZxTzevZEvmvLGIlDgl+GkyM143Okx7RRXDmePmnuMiUsSU4GfgxLIo2UiEbcmRQociInJESvAzcHJjPQDPd6onjYjMf0rwM/CGZUuIZDJs3tNZ6FBERI5ICX4Gak96A22dHWzpHSh0KCIiR6QEPwOxlhZWdu/jRfWkEZEioAQ/A2bGadkU7dU1HNDQwSIyzynBz9CaumoAntyrE60iMr8pwc/QmSuXY9ks//byq4UORURkSkrwM9Sy+o0s27uHp7pfc/dBEZF5RQl+hqK1tazq7WJjWSVZ90KHIyIyKSX4o/C7lXF6K6p4Rhc8icg8pgR/FC456QQAfv7slgJHIiIyOSX4o7Bi7RpWduziV939hQ5FRGRSSvBHweJxLhgd4smGZrr7lORFZH5Sgj9KV520knQsxvd/+VihQxERmZAS/FE6+6w1tPV0cU/XAK7eNCIyDynBH6VIJMJ7KqJsWP46NvzswUKHIyLyGkrwx2DdRedQMZriS5tfJjs8XOhwREQOowR/DJoqK/iDigg/XbWGh7/45UKHIyJyGCX4Y/Tpc1fTkEnz54tO4NUvfVnt8SIybyjBH6O6eIy/W30i29qW88nRONv/5OOM7tlT6LBERIgVOoBS8PaWBv76xDQ3mvHR+kb+9EPXsPrsM6l/z3uoXL0aM90gRETmns2nJoW1a9f6+vXrCx3GUftRZw9/tvkV+jIZfu+p33LxE49wdl8XLW96E9Xnn0f1uecSTSQKHaaIlBAz2+DuaydcpgQ/u7pSaf7h1b3csbuL/kwWgOaebto6drN0Xwevi0dYtWQRZ5z8elpPOZloTU2BIxaRYqYEXwCpbJYnegZ5sm+IlwaH2Lr/ANtSGfpj8YNlWrv28YZ9HZw6PMhpceN36hMsWbKE8uXLKWtbgpWVFfAViEgxmCrBqw0+T8oiES5orOGCxrEa+krcnf2jaZ7Z3cnTO3byTDzC5rblPFJdg4ft9A19vaz46WMs6exgWXKAFWRZWR5jZX0t1YsWEV+8mPiSJcQXLyJSUVG4Fygi855q8PPAQDrDpv4hnuns5ul93byUHOEVovTk1PYj2Swt3ftp6+xgyb4O2jo7WJYcZEXUWF5TReXChcQXLSTWeuhvbEETFo0W8JWJSL6pBj/PJWJRzmmo4ZyGGjhp+cH5B0bTvDw0wvbkCNsHk2yrq2R7SzMPZlbRb4d6uEazGRZ27aNt7x6WbN0Q/Ah0dtDWvY8lsSgVrS3EFy4ktnBh+LeV+KJFxFpbiS1YgEXUW1akFOUtwZvZ7cAVQKe7r8rXfkpZQzxGQ12MNXXV4ZwlALg7XaMZtg8Nsz05wsvJFNtbG9m+bCkPDI8ylHNQFstmWTA0QGNvDy2dHSzZ9BKLfvkbFu7vZGHXPloG+qhqajo8+S9cdPBvfGEr0aYm/QiIFKF81uC/AXwJ+GYe93FcMjMWlMVYUJbg7PrDu126O52pdFDrHxrh1eEUu0dS7B1ZzCvDr+fXyRSZnPIRd5qHh1jUe4CF+/bSumsnrc8/Qmv3/uAHoLuLWDRCvKWF2KKFQfJvbQme5z6am3VOQGSeyVuCd/dfmdmKfG1fJmZmtJbHaS2Pc279a/vcp7PO7pEUO4dTvDoc/N05nGJnsoVnl6/kJ79zJrlnZcydltERFg70sbC7i+bOPTRve4XWxzfQ0t1Fa/d+qkaCgdYitbXEWpqD5N/cQqylOfw79mgOfgjKy+fo3RA5vhW8Dd7M1gHrAJYtW1bgaEpfLGIsqyxnWWU5502wPJXNsmdk9NAPQDJF+0iKXcML2Ly4jV+MnEJ63Hn5umyGRalhFg7009rTRWtnBwt272TBpl/Tuq+D+v4+Ijkn86N1dYfV/HN/AOItLUQXNBNraiRSWZnfN0OkxOW1F01Yg79vum3wx2svmmKScaczNUr78Ci7hlO0h0cAu8IfhV3DKQbCC7zGlAELybJwdISFQ/209BygeX8nCzr20LjzFRpe3kZtXy/jB3SwqipijY1EGxuDv02NxBqbgudNjUQbm8K/jcQaGnTdgByX1ItGZk3UjEXlZSwqL+Osgyd/D3F3etMZdo+M0h4m/l1h4t81MsqG4Vr21LWQXX7SYeuVG7RGoDWTpmU4GfwQ9PbQ3LWPhs69NO7ZRe2WLbB/P6TTE8YWqa0l1tBApL6OaG0d0bo6orW1ROpqg+e1tUTrw3m1dUTraonW1WEVFRovSEqSErzMKjOjPh6jPh7j1MTETSzprNORGmXPSPDoGEnlTI/ybEUVP62sZbRxMazM2TbQXBZjYSxCs2dpSqdoHk7SODhAQ98BGrq7qNvfSX1nJxVd+0m98gqZ3l6yfX0wxZGqxeNEamuJVFcTqaqa3t/KSqy8DCsrI1IW/LXy8uBvWRkWLyMSLreyMiymfzWZe/nsJnkXcBGwwMzagf/p7rfla39SPGIRo62ijLaKyZtUsu7sT6VpH0mxL5WmY2SUvalDPwIdqVGezcC+aASvrYbaVmg7tH55xFgQj9FUFqM5HqPJnIZMhrrRERpHktQnh6jr76eh9wC1Pd1U9PTgQ0NkBwfJDg2ROXCA0V27Dj7PDg5CNjtpvEcUieT8AMSJxCf4UYhGIRrBIlGIRYO/Y8+jESwaw6IRyJlvsShEogfnWzQCOeUsGgGLhOWD6YPzIhGIWLD9iAXLc6fHyr5m2oJYLXLYehaxcJvhvqZY71DZydYLy2KpBwQAAA1kSURBVEYiwdFV7vS4h46+JpfPXjQfyNe2pfRFzGgpj9NSHp+yXDrrdI2m2ZcaZf9omn2pNPtT4d/RUfaF08+PpukeTTOSdaAc4uXQ2ACNwYn9ikjQ9bQyEmHr0Mhr9vM7iUoub6jmQHKYleYszoxi6TQ7U2ku9BTlqRSJ1DDR1CieSoWPETyVIjv2fCSVsyyFj6bI5s7LpGF0lGw2A5ksnslAJoOPPc9mIJ3Bs9lwfvj3YLmc+ZM0Y5W0iZJ/7vyxH44Jyk70wxE8wDiG+ZFIMM8McucfFgPE6htYdvvs13913ChFLRY51C30SNydoWyWrlSartHMwR+FA2PTqTQPd/cDwRFA8GMQeHYgybMDyQm2GgEqgke8FuJAeGpiSXmcvalR0g6N8Sjn1ieojkb4TscBAC5prKG1PM5de7oBeGNNJbuGR3lDdTndoxleHBymMR7jnS31tJTFWFJRxnc6umkti3NlSz2P9wyyrLKMwUyW361PsHskxYbeId6zsIG0O+3JERZEI7QPp3hxcJgY8OzgMImI8c76Kj6yvZM0cM/KBVQZZDMZyoByHMtkSGWdfekMSyLw6HCaU6JQj/P9ZIY3RZ0286B3VFi2PQuJbIadWVhGhvuzMd7sKRZk03Q7VGUzWNaJZ7Ok3anMpKnIZsGzRDNpPAvmGTzrkMkSz2YYdYhl0ng2i7tj2QxZdzIOEYdYNkMGyALRbJaIZ8m6MWqGZbNUeIa0QxaHrBPNZsGdZDRKxp2adJqsO1mCbeHOC1W1vG6wj1i4z5GIUZbJ4O5BS5+DebCdCB7cxc0Bd1IWlAUn7WDZLOZhGRwbayp0wu0F72GkNj+jymosGpEpePjPP5zJsn80zavJFOURI2JGx8gon92+m7c21fG19n2cWVvFsooy7unsAeDixhp60xme7BsC4MSqcvrTWTpSowV8RTJfbbvgd6iOzXzsKPWiETlKZkYUqI5FqY5FWV55+EVaV7TUA/CXJy45OO8rp01/+x7WRtPuJLNZhjJZRt3ZPjRCfybDQDrL4vI4WaAyEuGnXb20VZRxanUl/7qvh+ayGGmH02sq2dg/xC2vdPKVU5eTzGb59YF+ViUqWZWo4oneARwYSGeJGZxbn+CaTTtIZrP85esX01ZRRiobHOFECCqk7cPBEcE7Wur4zIvtfGxpC4vK49y4dRcAN7+hjZgZjrMvleZvX+7gzU21/Lyrj6taG/j+3uBI5R3NdTzeM0jX6GubjZrLYlzV2sAvuvqoiUbpTWfYngyayKIGmRnWP6ujEQZzuuk2xqN0j2YmLX9GTRVP9Q9NuKwqGmEocwznXWboaJL7kagGLyJSxKaqwWsEKRGREqUELyJSopTgRURKlBK8iEiJUoIXESlRSvAiIiVKCV5EpEQpwYuIlCgleBGREqUELyJSopTgRURKlBK8iEiJUoIXESlRSvAiIiVKCV5EpEQpwYuIlCgleBGREqUELyJSopTgRURKlBK8iEiJUoIXESlRSvAiIiVKCV5EpEQpwYuIlCgleBGREqUELyJSopTgRURKlBK8iEiJUoIXESlRSvAiIiVKCV5EpETlNcGb2aVm9oKZvWRm1+dzXyIicri8JXgziwL/F7gMOBX4gJmdmq/9iYjI4fJZgz8beMndt7t7CrgbeGce9yciIjliedz2EmBnzvN24E3jC5nZOmBd+HTAzF44yv0tAPYf5br5pLhmRnHNjOKamVKMa/lkC/KZ4KfF3b8GfO1Yt2Nm69197SyENKsU18worplRXDNzvMWVzyaaXcDSnOdt4TwREZkD+Uzw/wacaGYrzawMeD/wwzzuT0REcuSticbd02b2X4CfAFHgdnd/Ll/7YxaaefJEcc2M4poZxTUzx1Vc5u752K6IiBSYrmQVESlRSvAiIiWq6BP8XA+HYGa3m1mnmW3KmddoZj8zs63h34ZwvpnZLWFsz5jZmpx1PhSW32pmH5qFuJaa2UNm9ryZPWdm182H2Myswsx+a2ZPh3H9RTh/pZk9Ee7/2+GJeMysPHz+Urh8Rc62PhPOf8HM3n4sceVsM2pmT5nZffMlLjPbYWbPmtlGM1sfzpsP37F6M/sXM9tiZpvN7NxCx2VmJ4Xv09ijz8w+Uei4wu19MvzObzKzu8L/hbn9frl70T4ITt5uA04AyoCngVPzvM8LgTXAppx5nweuD6evB/5XOH058GPAgHOAJ8L5jcD28G9DON1wjHEtAtaE0zXAiwRDRBQ0tnD7iXA6DjwR7u87wPvD+bcC/ymcvha4NZx+P/DtcPrU8PMtB1aGn3t0Fj7P/wrcCdwXPi94XMAOYMG4efPhO/b/gI+G02VA/XyIKye+KNBBcOFPob/3S4CXgcqc79WH5/r7NStJr1AP4FzgJznPPwN8Zg72u4LDE/wLwKJwehHwQjj9VeAD48sBHwC+mjP/sHKzFOO9wFvnU2xAFfAkwRXN+4HY+M+RoNfVueF0LCxn4z/b3HLHEE8b8AvgEuC+cD/zIa4dvDbBF/RzBOoIEpbNp7jGxfI24NH5EBeHruRvDL8v9wFvn+vvV7E30Uw0HMKSAsTR6u57wukOoDWcniy+vMYdHt6dQVBbLnhsYTPIRqAT+BlBLaTH3dMT7OPg/sPlvUBTPuIC/h74MyAbPm+aJ3E58FMz22DBUB5Q+M9xJbAP+HrYpPVPZlY9D+LK9X7grnC6oHG5+y7gC8CrwB6C78sG5vj7VewJft7x4Ge2YH1PzSwBfA/4hLv35S4rVGzunnH31QQ15rOBk+c6hvHM7Aqg0903FDqWCZzv7msIRmL9z2Z2Ye7CAn2OMYKmya+4+xnAIEHTR6HjAiBsy74S+O74ZYWIK2zzfyfBD+NioBq4dC5jgOJP8PNlOIS9ZrYIIPzbGc6fLL68xG1mcYLkfoe7f38+xQbg7j3AQwSHpvVmNnahXe4+Du4/XF4HdOUhrvOAK81sB8FIp5cAX5wHcY3V/nD3TuAegh/FQn+O7UC7uz8RPv8XgoRf6LjGXAY86e57w+eFjustwMvuvs/dR4HvE3zn5vT7VewJfr4Mh/BDYOys+4cI2r/H5v+H8Mz9OUBveNj4E+BtZtYQ/tK/LZx31MzMgNuAze7+d/MlNjNrNrP6cLqS4LzAZoJE/95J4hqL973Ag2EN7IfA+8PeBiuBE4HfHm1c7v4Zd29z9xUE35sH3f3fFzouM6s2s5qxaYL3fxMF/hzdvQPYaWYnhbPeDDxf6LhyfIBDzTNj+y9kXK8C55hZVfi/OfZ+ze33azZObhTyQXBW/EWCdt0b52B/dxG0qY0S1Go+QtBW9gtgK/BzoDEsawQ3PdkGPAuszdnONcBL4eOPZiGu8wkOQ58BNoaPywsdG/BG4Kkwrk3An4fzTwi/qC8RHFaXh/MrwucvhctPyNnWjWG8LwCXzeJnehGHetEUNK5w/0+Hj+fGvtOF/hzD7a0G1oef5Q8IepvMh7iqCWq7dTnz5kNcfwFsCb/3/0zQE2ZOv18aqkBEpEQVexONiIhMQgleRKREKcGLiJQoJXgRkRKlBC8iUqKU4GXeMrMmOzRKYIeZ7cp5XnaEddea2S3T2Mdjsxfxa7Zdb2bX5mv7IkeibpJSFMzsJmDA3b+QMy/mh8b1mHfCMYHuc/dVBQ5FjlOqwUtRMbNvmNmtZvYE8HkzO9vMfhMOgPXY2JWWZnaRHRrj/SYLxvF/2My2m9nHc7Y3kFP+YTs03vkd4RWImNnl4bwNFowlft8EcZ1mwbj3Gy0YZ/xE4GbgdeG8vw3L/amZ/VtYZmxs/BU5+9wcxlAVLrvZgjH+nzGzL4zfr8hU8nbTbZE8agN+190zZlYLXODBTd7fAnwOeM8E65wMXEwwVv4LZvYVD8YIyXUGcBqwG3gUOM+CG258FbjQ3V82s7uY2MeAL7r7HWHzUZRgMK5VHgy0hpm9jeBS87MJrqj8oQUDib0KnAR8xN0fNbPbgWvN7OvAu4GT3d3HhnwQmS7V4KUYfdfdM+F0HfBdC+6w9X8IEvRE/tXdR9x9P8HAU60TlPmtu7e7e5ZgqIcVBD8M29395bDMZAn+N8ANZvZpYLm7Jyco87bw8RTBuPgnEyR8gJ3u/mg4/S2CoSd6gWHgNjO7ChiaZN8iE1KCl2I0mDP9V8BDYTv3vyMY02MiIznTGSY+ep1OmQm5+50Ew9UmgfvN7JIJihnwN+6+Ony83t1vG9vEazfpaYLa/r8AVwAPTDceEVCCl+JXx6HhUz+ch+2/AJxgh+6RefVEhczsBIKa/i0EIwS+EegnaBIa8xPgGgvG7MfMlphZS7hsmZmdG07/AfBIWK7O3e8HPgmcPmuvSo4LSvBS7D4P/I2ZPUUezimFTS3XAg+Y2QaCpN07QdHfBzZZcOeqVcA33b0LeNSCmy7/rbv/lOD+r78xs2cJauZjPwAvENzcYzPBKI1fCZfdZ2bPAI8Q3D9WZNrUTVLkCMws4e4DYa+a/wtsdff/M4vbX4G6U0oeqAYvcmR/HNbMnyNoEvpqgeMRmRbV4EVESpRq8CIiJUoJXkSkRCnBi4iUKCV4EZESpQQvIlKi/j8N4fw1mZRT1wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "z5zKRU9Yq4s-",
        "outputId": "b71caf9f-9f64-4129-bef5-a7d30134388e"
      },
      "source": [
        "del model\n",
        "model = NeuralNet(tr_set.dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "plot_pred(dv_set, model, device)  # Show prediction on the validation set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfPzzjsFruF4"
      },
      "source": [
        "### Start Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxWI7iiOrqLj",
        "outputId": "33a176b0-a608-481a-9a9e-f5d8e78c53d1"
      },
      "source": [
        "def save_pred(preds, file):\n",
        "    ''' Save predictions to specified file '''\n",
        "    print('Saving results to {}'.format(file))\n",
        "    with open(file, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'tested_positive'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i, p])\n",
        "\n",
        "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to pred.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "410gsmOmso9h"
      },
      "source": [
        "# 参考的 hw1_best"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha1byciyslk-"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "def set_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "set_seeds(324)\n",
        "\n",
        "tr_path = 'covid.train.csv'  # path to training data\n",
        "tt_path = 'covid.test.csv'   # path to testing data\n",
        "\n",
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def plot_learning_curve(loss_record, title=''):\n",
        "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "    total_steps = len(loss_record['train'])\n",
        "    x_1 = range(total_steps)\n",
        "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
        "    figure(figsize=(6, 4))\n",
        "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
        "    plt.ylim(0.0, 5.)\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('MSE loss')\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.legend()\n",
        "    plt.savefig('ori_modi.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n",
        "    ''' Plot prediction of your DNN '''\n",
        "    if preds is None or targets is None:\n",
        "        model.eval()\n",
        "        preds, targets = [], []\n",
        "        for x, y in dv_set:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                pred = model(x)\n",
        "                preds.append(pred.detach().cpu())\n",
        "                targets.append(y.detach().cpu())\n",
        "        preds = torch.cat(preds, dim=0).numpy()\n",
        "        targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "    figure(figsize=(5, 5))\n",
        "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
        "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
        "    plt.xlim(-0.2, lim)\n",
        "    plt.ylim(-0.2, lim)\n",
        "    plt.xlabel('ground truth value')\n",
        "    plt.ylabel('predicted value')\n",
        "    plt.title('Ground Truth v.s. Prediction')\n",
        "    plt.show()\n",
        "\n",
        "class COVID19Dataset(Dataset):\n",
        "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
        "    def __init__(self,\n",
        "                 path,\n",
        "                 mode='train',\n",
        "                 target_only=True):\n",
        "        self.mode = mode\n",
        "\n",
        "        # Read data into numpy arrays\n",
        "        with open(path, 'r') as fp:\n",
        "            data = list(csv.reader(fp))\n",
        "            data = np.array(data[1:])[:, 1:].astype(float)\n",
        "        \n",
        "        if not target_only:\n",
        "            feats = list(range(93))\n",
        "        else:\n",
        "        \tfeats = []\n",
        "        \tfeats.append(40)\n",
        "        \tfeats.append(41)\n",
        "        \tfeats.append(42)\n",
        "        \tfeats.append(43)\n",
        "        \tfeats.append(55)\n",
        "        \tfeats.append(57)\n",
        "        \tfeats.append(58)\n",
        "        \tfeats.append(59)\n",
        "        \tfeats.append(60)\n",
        "        \tfeats.append(61)\n",
        "        \tfeats.append(73)\n",
        "        \tfeats.append(75)\n",
        "        \tfeats.append(76)\n",
        "        \tfeats.append(77)\n",
        "        \tfeats.append(78)\n",
        "        \tfeats.append(79)\n",
        "        \tfeats.append(91)\n",
        "        if mode == 'test':\n",
        "            # Testing data\n",
        "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
        "            data = data[:, feats]\n",
        "            self.data = torch.FloatTensor(data)\n",
        "        else:\n",
        "            # Training data (train/dev sets)\n",
        "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
        "            target = data[:, -1]\n",
        "            data = data[:, feats]\n",
        "            \n",
        "            # Splitting training data into train & dev sets\n",
        "            if mode == 'train':\n",
        "                indices = [i for i in range(len(data)) if i % 10 != 1]\n",
        "            elif mode == 'dev':\n",
        "                indices = [i for i in range(len(data)) if i % 10 == 1]\n",
        "            \n",
        "            # Convert data into PyTorch tensors\n",
        "            self.data = torch.FloatTensor(data[indices])\n",
        "            self.target = torch.FloatTensor(target[indices])\n",
        "\n",
        "        # Normalize features (you may remove this part to see what will happen)\n",
        "\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\n",
        "              .format(mode, len(self.data), self.dim))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)\n",
        "\n",
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=True):\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size,\n",
        "        shuffle=(mode == 'train'), drop_last=False,\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
        "    return dataloader\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        # Define your neural network here\n",
        "        # TODO: How to modify this model to achieve better performance?\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            #nn.Dropout(0.05),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        # TODO: you may implement L2 regularization here\n",
        "        return self.criterion(pred, target)\n",
        "def train(tr_set, dv_set, model, config, device):\n",
        "    ''' DNN training '''\n",
        "\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "\n",
        "    min_mse = 1000\n",
        "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "    train_mse = 0\n",
        "    while epoch < n_epochs:\n",
        "        model.train()                           # set model to training mode\n",
        "        for x, y in tr_set:                     # iterate through the dataloader\n",
        "            optimizer.zero_grad()               # set gradient to zero\n",
        "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
        "            optimizer.step()                    # update model with optimizer\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "            train_mse = mse_loss.detach().cpu().item()\n",
        "        # After each epoch, test your model on the validation (development) set.\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        \n",
        "        if dev_mse < min_mse:\n",
        "            # Save model if your model improved\n",
        "            min_mse = dev_mse\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "        \n",
        "        if epoch%10 == 0:\n",
        "            print('epoch = {:4d}, train_mse = {:.4f}, dev_mse = {:.4f}'.format(epoch, train_mse, dev_mse))\n",
        "        epoch += 1\n",
        "        #loss_record['dev'].append(dev_mse)\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
        "            break\n",
        "\n",
        "    print('Finished training after {} epochs'.format(epoch))\n",
        "    #torch.save(model.state_dict(), config['save_path'])\n",
        "    return min_mse, loss_record\n",
        "\n",
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\n",
        "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds\n",
        "\n",
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models0', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "target_only = True                   # TODO: Using 40 states & 2 tested_positive features\n",
        "\n",
        "\n",
        "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
        "config = {\n",
        "    'n_epochs': 8000,                # maximum number of epochs\n",
        "    'batch_size': 2430,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.0005,                \n",
        "    },\n",
        "    'early_stop': 5000,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
        "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
        "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)\n",
        "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device\n",
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdSpYn3JssmI"
      },
      "source": [
        "df = pd.read_csv(\"covid.train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0D45a0pntO0w",
        "outputId": "0e6af006-bf49-4c76-f692-d3583ed5496f"
      },
      "source": [
        "tr_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'covid.train.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}